[
  {
    "objectID": "contents/1/intro.html#why-python",
    "href": "contents/1/intro.html#why-python",
    "title": "1  Preliminaries",
    "section": "1.1 Why Python?",
    "text": "1.1 Why Python?\n\n1.1.1 Python is easy to learn and use\n\n\n\n1.1.2 Python is easy to read\n\n\n1.1.3 Python Community is mature and supportive",
    "crumbs": [
      "Part I: Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preliminaries</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#hello-world",
    "href": "contents/1/intro.html#hello-world",
    "title": "1  Preliminaries",
    "section": "1.2 Hello world!",
    "text": "1.2 Hello world!\n\n\n1.2.1 Setup the Python environment\nIn this section we are going to setup the Python developing environment.\n\n1.2.1.1 VS Code + Anaconda\nClick ?sec-vscode to see the detailed steps for VS Code and conda. You may also check out the official document. It contains more features but less details.\nWe will talk about the relation between Python and Anaconda and more about packages sometime later.\n\n\n1.2.1.2 Google Colab\nClick ?sec-googlecolab for more details.\n\n\n\n1.2.2 Hello World!\n\nTake VS Code as an example. In the editor window, type in the code, and run the file in the interactive window.\n\nprint('Hello World!')\n\n\n\n\n\n\nIf you see a small green check mark in the interactive window and also the output Hello World!, you are good to go!\n\n\n1.2.3 Python code cells and Notebooks\nIn VS Code you can run codes cell by cell. Each cell is separated by the symbol # %%. Each cell may contain multiple lines. You may click the small button on top of the cell or use keybindings.\n\n\n\n\n\nThis feature actually mimic the notebook. We may start a real Python Notebook file by directly creating a file with extension .ipynb.\n\n\n\n\n\nThe layout is straightforward.\n\n\n1.2.4 Linters\nA linter is a tool to help you improve your code by analyzing your source code looking for problems. Some popular choices for Python includes Flake8 and Pylint. It is highly recommended to use one that is compatible with your IDE which can help you to increase the quality of your codes from the begining.\nTo install Flake8 please go to its homepage. It works well with VS Code.",
    "crumbs": [
      "Part I: Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preliminaries</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#exercises",
    "href": "contents/1/intro.html#exercises",
    "title": "1  Preliminaries",
    "section": "1.3 Exercises",
    "text": "1.3 Exercises\n\nExercise 1.1 (Hello world!) Please set up a Python developing environment, for both .py file and .ipynb notebook file, that will be used across the semester. Then print Hello World!.\n\n\nExercise 1.2 (Define a function and play with time) Please play with the following codes in a Jupyter notebook. We haven’t talked about any of them right now. Try to guess what they do and write your guess in markdown cells.\n\nimport time\n\ndef multistr(x, n=2):\n    return x * n\n\nt0 = time.time()\nx = 'Python'\nprint(multistr(x, n=10))\nt1 = time.time()\nprint(\"Time used: \", t1-t0)\n\n\n\nExercise 1.3 (Fancy Basketball plot) Here is an example of the data analysis. We pull data from a dataset, filter the data according to our needs and plot it to visualize the data. This is just a show case. You are encouraged to play the code, make tweaks and see what would happen. You don’t have to turn in anything.\nThe data we choose is Stephen Curry’s shots data in 2021-2022 regular season. First we need to load the data. The data is obtained from nba.com using nba_api. To install this package please go to its PyPI page.\n\nfrom nba_api.stats.static import players\nfrom nba_api.stats.endpoints import shotchartdetail\nplayer_dict = players.get_players()\n\nThe shots data we need is in shotchartdetail. However to use it we need to know the id of Stephen Curry using the dataset player_dict.\n\nfor player in player_dict:\n    if player['full_name'] == 'Stephen Curry':\n        print(player['id'])\n\n201939\n\n\nSo the id of Stephen Curry is 201939. Let’s pull out his shots data in 2021-2022 season.\n\nresults = shotchartdetail.ShotChartDetail(\n            team_id = 0,\n            player_id = 201939,\n            context_measure_simple = 'FGA',\n            season_nullable = '2021-22',\n            season_type_all_star = 'Regular Season')\ndf = results.get_data_frames()[0]\ndf.head()\n\n\n\n\n\n\n\n\nGRID_TYPE\nGAME_ID\nGAME_EVENT_ID\nPLAYER_ID\nPLAYER_NAME\nTEAM_ID\nTEAM_NAME\nPERIOD\nMINUTES_REMAINING\nSECONDS_REMAINING\n...\nSHOT_ZONE_AREA\nSHOT_ZONE_RANGE\nSHOT_DISTANCE\nLOC_X\nLOC_Y\nSHOT_ATTEMPTED_FLAG\nSHOT_MADE_FLAG\nGAME_DATE\nHTM\nVTM\n\n\n\n\n0\nShot Chart Detail\n0022100002\n26\n201939\nStephen Curry\n1610612744\nGolden State Warriors\n1\n10\n9\n...\nLeft Side Center(LC)\n24+ ft.\n28\n-109\n260\n1\n0\n20211019\nLAL\nGSW\n\n\n1\nShot Chart Detail\n0022100002\n34\n201939\nStephen Curry\n1610612744\nGolden State Warriors\n1\n9\n41\n...\nCenter(C)\n24+ ft.\n26\n48\n257\n1\n0\n20211019\nLAL\nGSW\n\n\n2\nShot Chart Detail\n0022100002\n37\n201939\nStephen Curry\n1610612744\nGolden State Warriors\n1\n9\n10\n...\nLeft Side Center(LC)\n24+ ft.\n25\n-165\n189\n1\n1\n20211019\nLAL\nGSW\n\n\n3\nShot Chart Detail\n0022100002\n75\n201939\nStephen Curry\n1610612744\nGolden State Warriors\n1\n6\n17\n...\nCenter(C)\nLess Than 8 ft.\n1\n-13\n12\n1\n0\n20211019\nLAL\nGSW\n\n\n4\nShot Chart Detail\n0022100002\n130\n201939\nStephen Curry\n1610612744\nGolden State Warriors\n1\n3\n11\n...\nCenter(C)\nLess Than 8 ft.\n2\n-15\n22\n1\n0\n20211019\nLAL\nGSW\n\n\n\n\n5 rows × 24 columns\n\n\n\ndf is the results we get in terms of a DataFrame, and we show the first 5 records as an example.\nThese are all attempts. We are interested in all made. By looking at all the columns, we find a column called SHOT_MADE_FLAG which shows what we want. Therefore we will use it to filter the records.\n\ndf_made = df[df['SHOT_MADE_FLAG']==1]\ndf_made.head()\n\n\n\n\n\n\n\n\nGRID_TYPE\nGAME_ID\nGAME_EVENT_ID\nPLAYER_ID\nPLAYER_NAME\nTEAM_ID\nTEAM_NAME\nPERIOD\nMINUTES_REMAINING\nSECONDS_REMAINING\n...\nSHOT_ZONE_AREA\nSHOT_ZONE_RANGE\nSHOT_DISTANCE\nLOC_X\nLOC_Y\nSHOT_ATTEMPTED_FLAG\nSHOT_MADE_FLAG\nGAME_DATE\nHTM\nVTM\n\n\n\n\n2\nShot Chart Detail\n0022100002\n37\n201939\nStephen Curry\n1610612744\nGolden State Warriors\n1\n9\n10\n...\nLeft Side Center(LC)\n24+ ft.\n25\n-165\n189\n1\n1\n20211019\nLAL\nGSW\n\n\n6\nShot Chart Detail\n0022100002\n176\n201939\nStephen Curry\n1610612744\nGolden State Warriors\n1\n0\n27\n...\nCenter(C)\nLess Than 8 ft.\n2\n-7\n29\n1\n1\n20211019\nLAL\nGSW\n\n\n9\nShot Chart Detail\n0022100002\n352\n201939\nStephen Curry\n1610612744\nGolden State Warriors\n2\n1\n29\n...\nCenter(C)\nLess Than 8 ft.\n1\n-1\n10\n1\n1\n20211019\nLAL\nGSW\n\n\n16\nShot Chart Detail\n0022100002\n510\n201939\nStephen Curry\n1610612744\nGolden State Warriors\n3\n2\n23\n...\nCenter(C)\nLess Than 8 ft.\n1\n7\n8\n1\n1\n20211019\nLAL\nGSW\n\n\n18\nShot Chart Detail\n0022100002\n642\n201939\nStephen Curry\n1610612744\nGolden State Warriors\n4\n5\n34\n...\nCenter(C)\n24+ ft.\n26\n48\n260\n1\n1\n20211019\nLAL\nGSW\n\n\n\n\n5 rows × 24 columns\n\n\n\nWe also notice that there are two columns LOC_X and LOC_Y shows the coordinates of the attempts. We will use it to draw the heatmap. The full code for drawing out the court draw_court is folded below. It is from Bradley Fay GitHub.\n\n\n\n\n\n\nNote\n\n\n\nNote that, although draw_cort is long, it is not hard to understand. It just draws a court piece by piece.\n\n\n\n\nCode\nfrom matplotlib.patches import Circle, Rectangle, Arc\nimport matplotlib.pyplot as plt\n\n\ndef draw_court(ax=None, color='gray', lw=1, outer_lines=False):\n    \"\"\"\n    Returns an axes with a basketball court drawn onto to it.\n\n    This function draws a court based on the x and y-axis values that the NBA\n    stats API provides for the shot chart data.  For example, the NBA stat API\n    represents the center of the hoop at the (0,0) coordinate.  Twenty-two feet\n    from the left of the center of the hoop in is represented by the (-220,0)\n    coordinates.  So one foot equals +/-10 units on the x and y-axis.\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    # Create the various parts of an NBA basketball court\n\n    # Create the basketball hoop\n    hoop = Circle((0, 0), radius=7.5, linewidth=lw, color=color, fill=False)\n\n    # Create backboard\n    backboard = Rectangle((-30, -7.5), 60, -1, linewidth=lw, color=color)\n\n    # The paint\n    # Create the outer box 0f the paint, width=16ft, height=19ft\n    outer_box = Rectangle((-80, -47.5), 160, 190, linewidth=lw, color=color,\n                          fill=False)\n    # Create the inner box of the paint, widt=12ft, height=19ft\n    inner_box = Rectangle((-60, -47.5), 120, 190, linewidth=lw, color=color,\n                          fill=False)\n\n    # Create free throw top arc\n    top_free_throw = Arc((0, 142.5), 120, 120, theta1=0, theta2=180,\n                         linewidth=lw, color=color, fill=False)\n    # Create free throw bottom arc\n    bottom_free_throw = Arc((0, 142.5), 120, 120, theta1=180, theta2=0,\n                            linewidth=lw, color=color, linestyle='dashed')\n    # Restricted Zone, it is an arc with 4ft radius from center of the hoop\n    restricted = Arc((0, 0), 80, 80, theta1=0, theta2=180, linewidth=lw,\n                     color=color)\n\n    # Three point line\n    # Create the right side 3pt lines, it's 14ft long before it arcs\n    corner_three_a = Rectangle((-220, -47.5), 0, 140, linewidth=lw,\n                               color=color)\n    # Create the right side 3pt lines, it's 14ft long before it arcs\n    corner_three_b = Rectangle((220, -47.5), 0, 140, linewidth=lw, color=color)\n    # 3pt arc - center of arc will be the hoop, arc is 23'9\" away from hoop\n    three_arc = Arc((0, 0), 475, 475, theta1=22, theta2=158, linewidth=lw,\n                    color=color)\n\n    # Center Court\n    center_outer_arc = Arc((0, 422.5), 120, 120, theta1=180, theta2=0,\n                           linewidth=lw, color=color)\n    center_inner_arc = Arc((0, 422.5), 40, 40, theta1=180, theta2=0,\n                           linewidth=lw, color=color)\n\n    # List of the court elements to be plotted onto the axes\n    court_elements = [hoop, backboard, outer_box, inner_box, top_free_throw,\n                      bottom_free_throw, restricted, corner_three_a,\n                      corner_three_b, three_arc, center_outer_arc,\n                      center_inner_arc]\n\n    if outer_lines:\n        # Draw the half court line, baseline and side out bound lines\n        outer_lines = Rectangle((-250, -47.5), 500, 470, linewidth=lw,\n                                color=color, fill=False)\n        court_elements.append(outer_lines)\n\n    # Add the court elements onto the axes\n    for element in court_elements:\n        ax.add_patch(element)\n\n    return ax\n\n\n\n# Create figure and axes\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_axes([0, 0, 1, 1])\n\n# Plot hexbin of shots\nax.hexbin(df['LOC_X'], df['LOC_Y'], gridsize=(30, 30), extent=(-300, 300, 0, 940), bins='log', cmap='Blues')\nax = draw_court(ax, 'black')\n\n# Annotate player name and season\nax.text(0, 1.05, 'Stephen Curry\\n2021-22 Regular Season', transform=ax.transAxes, ha='left', va='baseline')\n\n# Set axis limits\n_ = ax.set_xlim(-250, 250)\n_ = ax.set_ylim(0, 400)",
    "crumbs": [
      "Part I: Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preliminaries</span>"
    ]
  },
  {
    "objectID": "contents/9/intro.html#tibble",
    "href": "contents/9/intro.html#tibble",
    "title": "9  R for Data Sciences",
    "section": "9.1 tibble",
    "text": "9.1 tibble\ntidyverse mainly deals with tibble instead of data.frame. Therefore this is where we start.\ntibble is a data.frame with different attributes and requirements. The package tibble provides support for tibble. It is included in tidyverse. To load it, you just use the code:\n\nlibrary(tidyverse)\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n9.1.1 Create tibbles\nHere is an example of creating tibbles.\n\nExample 9.1  \n\ntbl &lt;- tibble(x=1:5, y=1, z=x^2+y)\ntbl\n#&gt; # A tibble: 5 × 3\n#&gt;       x     y     z\n#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     1     2\n#&gt; 2     2     1     5\n#&gt; 3     3     1    10\n#&gt; 4     4     1    17\n#&gt; 5     5     1    26\nattributes(tbl)\n#&gt; $class\n#&gt; [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n#&gt; \n#&gt; $row.names\n#&gt; [1] 1 2 3 4 5\n#&gt; \n#&gt; $names\n#&gt; [1] \"x\" \"y\" \"z\"\n\nNote that it is more flexible to create a tibble since tibble() will automatically recycle inputs and allows you to refer to variables that you just created.\n\n\n\n\n\n\n\nNote\n\n\n\nIn the past (for a very long time), when using data.frame() to create a data.frame, it will automatically convert strings to factors. This is changed recently that the default setting is not to convert.\nWhen using tibble() to create a tibble, the type of the inputs will never be changed.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn tibble you may use nonsyntactic names as column names, which are invalid R variable names. To refer to these variables, you need to surround them with backticks `.\n\ntb &lt;- tibble(\n    `:)` = \"smile\",\n    ` ` = \"space\",\n    `2000` = \"number\"\n)\ntb\n#&gt; # A tibble: 1 × 3\n#&gt;   `:)`  ` `   `2000`\n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; \n#&gt; 1 smile space number\n\n\n\n\n\n9.1.2 Differences between tibble and data.frame.\n\n9.1.2.1 Printing\nTibbles have a refined print method that shows only the first 10 rows and all the columns that fit on screen.\n\ndeck &lt;- tibble(suit=rep(c('spades', 'hearts', 'clubs', 'diamonds'), 13), face=rep(1:13, 4))\ndeck\n#&gt; # A tibble: 52 × 2\n#&gt;    suit      face\n#&gt;    &lt;chr&gt;    &lt;int&gt;\n#&gt;  1 spades       1\n#&gt;  2 hearts       2\n#&gt;  3 clubs        3\n#&gt;  4 diamonds     4\n#&gt;  5 spades       5\n#&gt;  6 hearts       6\n#&gt;  7 clubs        7\n#&gt;  8 diamonds     8\n#&gt;  9 spades       9\n#&gt; 10 hearts      10\n#&gt; # ℹ 42 more rows\n\n\n\n9.1.2.2 Subsetting\nTo get a single value, [[]] or $ should be used, just like for data.frame. These two are almost the same. The only difference is that [[]] accepts positions, but $ only accepts names.\nTo be used in a pipe, the special placeholder . will be used.\n\ndeck %&gt;% .$face\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13  1  2  3  4  5  6  7  8  9 10 11 12\n#&gt; [26] 13  1  2  3  4  5  6  7  8  9 10 11 12 13  1  2  3  4  5  6  7  8  9 10 11\n#&gt; [51] 12 13\n\nWe will talk about pipes later.\n\n\n\n9.1.3 %&gt;% symbol\n%&gt;% is the pipeline symbol, which is another way to connect several functions. Most functions in tidyverse have the first argument data, and both the input data and the output are tibbles. The syntax here is that data %&gt;% function(arguments) is the same as function(data, arguments). The benefit is that it is easier to have many functions consecutively applied to the data. Please see the following example.\n\ndata %&gt;% function1(arguments1)\n    %&gt;% function2(arguments2)\n    %&gt;% function3(arguments3)\n    %&gt;% function4(arguments4)\n\nfunction4(function3(function2(function1(data, arguments1), arguments2), arguments3), arguments4)\n\ndata2 &lt;- function1(data, arguments1)\ndata3 &lt;- function2(data2, arguments2)\ndata4 &lt;- function3(data3, arguments3)\nfunction4(data4, arguments4)\n\nThe readability of the first one is much better than the second one. Comparing to the third one, we don’t need to create a lot of intermedia temporary variables.",
    "crumbs": [
      "Part II: R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R for Data Sciences</span>"
    ]
  },
  {
    "objectID": "contents/9/intro.html#tidy-data",
    "href": "contents/9/intro.html#tidy-data",
    "title": "9  R for Data Sciences",
    "section": "9.2 Tidy Data",
    "text": "9.2 Tidy Data\nThe same underlying data can be represented in multiple ways. The following example shows the same data organized in four different ways.\n\nExample 9.2 These tibbles are provided by tidyr. You could directly load it from tidyverse.\n\nlibrary(tidyverse)\ndata(table1, package='tidyr')\ndata(table2, package='tidyr')\ndata(table3, package='tidyr')\ndata(table4a, package='tidyr')\ndata(table4b, package='tidyr')\n\n\ntable1\n\n\ntable1\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year  cases population\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Afghanistan  1999    745   19987071\n#&gt; 2 Afghanistan  2000   2666   20595360\n#&gt; 3 Brazil       1999  37737  172006362\n#&gt; 4 Brazil       2000  80488  174504898\n#&gt; 5 China        1999 212258 1272915272\n#&gt; 6 China        2000 213766 1280428583\n\n\ntable2\n\n\ntable2\n#&gt; # A tibble: 12 × 4\n#&gt;    country      year type            count\n#&gt;    &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n#&gt;  1 Afghanistan  1999 cases             745\n#&gt;  2 Afghanistan  1999 population   19987071\n#&gt;  3 Afghanistan  2000 cases            2666\n#&gt;  4 Afghanistan  2000 population   20595360\n#&gt;  5 Brazil       1999 cases           37737\n#&gt;  6 Brazil       1999 population  172006362\n#&gt;  7 Brazil       2000 cases           80488\n#&gt;  8 Brazil       2000 population  174504898\n#&gt;  9 China        1999 cases          212258\n#&gt; 10 China        1999 population 1272915272\n#&gt; 11 China        2000 cases          213766\n#&gt; 12 China        2000 population 1280428583\n\n\ntable3\n\n\ntable3\n#&gt; # A tibble: 6 × 3\n#&gt;   country      year rate             \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n#&gt; 1 Afghanistan  1999 745/19987071     \n#&gt; 2 Afghanistan  2000 2666/20595360    \n#&gt; 3 Brazil       1999 37737/172006362  \n#&gt; 4 Brazil       2000 80488/174504898  \n#&gt; 5 China        1999 212258/1272915272\n#&gt; 6 China        2000 213766/1280428583\n\n\nSpread across two tibbles.\n\n\ntable4a\n#&gt; # A tibble: 3 × 3\n#&gt;   country     `1999` `2000`\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Afghanistan    745   2666\n#&gt; 2 Brazil       37737  80488\n#&gt; 3 China       212258 213766\n\ntable4b\n#&gt; # A tibble: 3 × 3\n#&gt;   country         `1999`     `2000`\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Afghanistan   19987071   20595360\n#&gt; 2 Brazil       172006362  174504898\n#&gt; 3 China       1272915272 1280428583\n\n\n\nDefinition 9.1 A dataset is tidy if\n\nEach variable have its own column.\nEach observation have its own row.\nEach value have its oven cell.\n\n\nThese three conditions are interrelated because it is impossible to only satisfy two of the three. In pratical, we need to follow the instructions:\n\nPut each dataset in a tibble.\nPut each variable in a column.\n\nTidy data is a consistent way to organize your data in R. The main advantages are:\n\nIt is one consistent way of storing data. In other words, this is a consistent data structure that can be used in many cases.\nTo placing variables in columns allows R’s vectorized nature to shine.\n\nAll packages in the tidyverse are designed to work with tidy data.\n\n9.2.1 Tidying datasets\nMost datasets are untidy:\n\nOne variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\n\n9.2.1.1 pivot_longer()\nA common problem is that the column names are not names of variables, but values of a variable. For example, table4a above has columns 1999 and 2000. These two names are actually the values of a variable year. In addition, each row represents two observations, not one.\n\ntable4a\n#&gt; # A tibble: 3 × 3\n#&gt;   country     `1999` `2000`\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Afghanistan    745   2666\n#&gt; 2 Brazil       37737  80488\n#&gt; 3 China       212258 213766\n\nTo tidy this type of dataset, we need to gather those columns into a new pair of variables. We need three parameters:\n\nThe set of columns that represent values. In this case, those are 1999 and 2000.\nThe name of the variable. In this case, it is year. -The name of the variable whose values are spread over the cells. In this case, it is the number of cases.\n\nThen we apply pivot_longer().\n\npivot_longer(table4a, cols=c(`1999`, `2000`), names_to='year', values_to='cases')\n#&gt; # A tibble: 6 × 3\n#&gt;   country     year   cases\n#&gt;   &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1 Afghanistan 1999     745\n#&gt; 2 Afghanistan 2000    2666\n#&gt; 3 Brazil      1999   37737\n#&gt; 4 Brazil      2000   80488\n#&gt; 5 China       1999  212258\n#&gt; 6 China       2000  213766\n\nWe may also use the pipe %&gt;% symbol.\n\ntable4a %&gt;% pivot_longer(cols=c(`1999`, `2000`), names_to='year', values_to='cases')\n#&gt; # A tibble: 6 × 3\n#&gt;   country     year   cases\n#&gt;   &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1 Afghanistan 1999     745\n#&gt; 2 Afghanistan 2000    2666\n#&gt; 3 Brazil      1999   37737\n#&gt; 4 Brazil      2000   80488\n#&gt; 5 China       1999  212258\n#&gt; 6 China       2000  213766\n\nWe can do the similar thing to table4b. Then we could combine the two tibbles together.\n\ntidy4a &lt;- table4a %&gt;% \n    pivot_longer(cols=c(`1999`, `2000`), names_to='year', values_to='cases')\ntidy4b &lt;- table4b %&gt;% \n    pivot_longer(cols=c(`1999`, `2000`), names_to='year', values_to='population')\nleft_join(tidy4a, tidy4b)\n#&gt; Joining with `by = join_by(country, year)`\n#&gt; # A tibble: 6 × 4\n#&gt;   country     year   cases population\n#&gt;   &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Afghanistan 1999     745   19987071\n#&gt; 2 Afghanistan 2000    2666   20595360\n#&gt; 3 Brazil      1999   37737  172006362\n#&gt; 4 Brazil      2000   80488  174504898\n#&gt; 5 China       1999  212258 1272915272\n#&gt; 6 China       2000  213766 1280428583\n\npivot_longer() is an updated approach to gather(), designed to be both simpler to use and to handle more use cases. We recommend you use pivot_longer() for new code; gather() isn’t going away but is no longer under active development.\n\n\n9.2.1.2 pivot_wider()\nAnother issuse is that an observation is scattered across multiple rows. Take table2 as an example. An observation is a country in a year, but each observation is spread across two rows.\n\ntable2\n#&gt; # A tibble: 12 × 4\n#&gt;    country      year type            count\n#&gt;    &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n#&gt;  1 Afghanistan  1999 cases             745\n#&gt;  2 Afghanistan  1999 population   19987071\n#&gt;  3 Afghanistan  2000 cases            2666\n#&gt;  4 Afghanistan  2000 population   20595360\n#&gt;  5 Brazil       1999 cases           37737\n#&gt;  6 Brazil       1999 population  172006362\n#&gt;  7 Brazil       2000 cases           80488\n#&gt;  8 Brazil       2000 population  174504898\n#&gt;  9 China        1999 cases          212258\n#&gt; 10 China        1999 population 1272915272\n#&gt; 11 China        2000 cases          213766\n#&gt; 12 China        2000 population 1280428583\n\nWe could apply pivot_wider() to make it tidy. Here we need two arguments.\n\nThe column that contains variable names. Here, it’s type.\nThe column that contains values forms multiple variables. Here, it’s count.\n\n\npivot_wider(table2, names_from='type', values_from='count')\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year  cases population\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Afghanistan  1999    745   19987071\n#&gt; 2 Afghanistan  2000   2666   20595360\n#&gt; 3 Brazil       1999  37737  172006362\n#&gt; 4 Brazil       2000  80488  174504898\n#&gt; 5 China        1999 212258 1272915272\n#&gt; 6 China        2000 213766 1280428583\n\nWe can also use the pipe symbol %&gt;%.\n\ntable2 %&gt;% pivot_wider(names_from='type', values_from='count')\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year  cases population\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Afghanistan  1999    745   19987071\n#&gt; 2 Afghanistan  2000   2666   20595360\n#&gt; 3 Brazil       1999  37737  172006362\n#&gt; 4 Brazil       2000  80488  174504898\n#&gt; 5 China        1999 212258 1272915272\n#&gt; 6 China        2000 213766 1280428583\n\npivot_wider() is an updated approach to spread(), designed to be both simpler to use and to handle more use cases. We recommend you use pivot_wider() for new code; spread() isn’t going away but is no longer under active development.\n\n\n9.2.1.3 separate()\nIf we would like to split one columns into multiple columns since there are more than one values in a cell, we could use separate().\n\nseparate(table3, rate, into=c('cases', 'population'))\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year cases  population\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n#&gt; 1 Afghanistan  1999 745    19987071  \n#&gt; 2 Afghanistan  2000 2666   20595360  \n#&gt; 3 Brazil       1999 37737  172006362 \n#&gt; 4 Brazil       2000 80488  174504898 \n#&gt; 5 China        1999 212258 1272915272\n#&gt; 6 China        2000 213766 1280428583\n\nWe could also use the pipe symbol %&gt;%.\n\ntable3 %&gt;% separate(rate, into=c('cases', 'population'))\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year cases  population\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n#&gt; 1 Afghanistan  1999 745    19987071  \n#&gt; 2 Afghanistan  2000 2666   20595360  \n#&gt; 3 Brazil       1999 37737  172006362 \n#&gt; 4 Brazil       2000 80488  174504898 \n#&gt; 5 China        1999 212258 1272915272\n#&gt; 6 China        2000 213766 1280428583\n\nUsing separate, the first argument is the column to be separated. into is where you store the parsed data. If no arguments are given, separate() will split values wherever it sees a non-alphanumeric character. If you would like to specify a separator, you may use the sep argument.\n\nIf sep is set to be a character, the column will be separated by the character.\nIf sep is set to be a vector of integers, the column will be separated by the positions.\n\n\nseparate(table3, rate, into=c('cases', 'population'), sep='/')\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year cases  population\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n#&gt; 1 Afghanistan  1999 745    19987071  \n#&gt; 2 Afghanistan  2000 2666   20595360  \n#&gt; 3 Brazil       1999 37737  172006362 \n#&gt; 4 Brazil       2000 80488  174504898 \n#&gt; 5 China        1999 212258 1272915272\n#&gt; 6 China        2000 213766 1280428583\n\n\nseparate(table3, rate, into=c('cases', 'population'), sep=c(2,5))\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year cases population\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     \n#&gt; 1 Afghanistan  1999 74    5/1       \n#&gt; 2 Afghanistan  2000 26    66/       \n#&gt; 3 Brazil       1999 37    737       \n#&gt; 4 Brazil       2000 80    488       \n#&gt; 5 China        1999 21    225       \n#&gt; 6 China        2000 21    376\n\nNote that in this example, since into only has two columns, the rest of the data are lost.\nAnother useful argument is convert. After separation, the columns are still character columns. If we set convert=TRUE, the columns will be automatically converted into better types if possible.\n\nseparate(table3, rate, into=c('cases', 'population'), convert=TRUE)\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year  cases population\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;\n#&gt; 1 Afghanistan  1999    745   19987071\n#&gt; 2 Afghanistan  2000   2666   20595360\n#&gt; 3 Brazil       1999  37737  172006362\n#&gt; 4 Brazil       2000  80488  174504898\n#&gt; 5 China        1999 212258 1272915272\n#&gt; 6 China        2000 213766 1280428583\n\n\n\n9.2.1.4 unite()\nunite() is the inverse of separate(). The syntax is straghtforward. The default separator is _.\n\ntable3 %&gt;% unite(new, year, rate, sep='_')\n#&gt; # A tibble: 6 × 2\n#&gt;   country     new                   \n#&gt;   &lt;chr&gt;       &lt;chr&gt;                 \n#&gt; 1 Afghanistan 1999_745/19987071     \n#&gt; 2 Afghanistan 2000_2666/20595360    \n#&gt; 3 Brazil      1999_37737/172006362  \n#&gt; 4 Brazil      2000_80488/174504898  \n#&gt; 5 China       1999_212258/1272915272\n#&gt; 6 China       2000_213766/1280428583",
    "crumbs": [
      "Part II: R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R for Data Sciences</span>"
    ]
  },
  {
    "objectID": "contents/9/intro.html#dplyr",
    "href": "contents/9/intro.html#dplyr",
    "title": "9  R for Data Sciences",
    "section": "9.3 dplyr",
    "text": "9.3 dplyr\ndplyr is a package used to manipulate data. Here we will just introduce the most basic functions. We will use nycflights13::flights as the example. This dataset comes from the US Bureau of Transportation Statistics. The document can be found here.\nTo load the dataset, please use the following code.\n\nlibrary(nycflights13)\nflights\n#&gt; # A tibble: 336,776 × 19\n#&gt;     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt;  1  2013     1     1      517            515         2      830            819\n#&gt;  2  2013     1     1      533            529         4      850            830\n#&gt;  3  2013     1     1      542            540         2      923            850\n#&gt;  4  2013     1     1      544            545        -1     1004           1022\n#&gt;  5  2013     1     1      554            600        -6      812            837\n#&gt;  6  2013     1     1      554            558        -4      740            728\n#&gt;  7  2013     1     1      555            600        -5      913            854\n#&gt;  8  2013     1     1      557            600        -3      709            723\n#&gt;  9  2013     1     1      557            600        -3      838            846\n#&gt; 10  2013     1     1      558            600        -2      753            745\n#&gt; # ℹ 336,766 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#&gt; #   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#&gt; #   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n9.3.1 filter()\nfilter() allows you to subset observations based on their values. The first argument is the name of the tibble. The rest are the expressions that filter the data. Please see the following examples.\n\nflights %&gt;% filter(month==1, day==1)\n#&gt; # A tibble: 842 × 19\n#&gt;     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt;  1  2013     1     1      517            515         2      830            819\n#&gt;  2  2013     1     1      533            529         4      850            830\n#&gt;  3  2013     1     1      542            540         2      923            850\n#&gt;  4  2013     1     1      544            545        -1     1004           1022\n#&gt;  5  2013     1     1      554            600        -6      812            837\n#&gt;  6  2013     1     1      554            558        -4      740            728\n#&gt;  7  2013     1     1      555            600        -5      913            854\n#&gt;  8  2013     1     1      557            600        -3      709            723\n#&gt;  9  2013     1     1      557            600        -3      838            846\n#&gt; 10  2013     1     1      558            600        -2      753            745\n#&gt; # ℹ 832 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#&gt; #   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#&gt; #   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n9.3.2 select()\nselect() allows you to filter columns. It is very similar to slicing [].\n\n\n9.3.3 mutate()\nmutate() is used to add new columns that are functions of existing columns.\n\nflights %&gt;% mutate(gain=arr_delay-dep_delay, hours=air_time/60, gain_per_hour=gain/hours)\n#&gt; # A tibble: 336,776 × 22\n#&gt;     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt;  1  2013     1     1      517            515         2      830            819\n#&gt;  2  2013     1     1      533            529         4      850            830\n#&gt;  3  2013     1     1      542            540         2      923            850\n#&gt;  4  2013     1     1      544            545        -1     1004           1022\n#&gt;  5  2013     1     1      554            600        -6      812            837\n#&gt;  6  2013     1     1      554            558        -4      740            728\n#&gt;  7  2013     1     1      555            600        -5      913            854\n#&gt;  8  2013     1     1      557            600        -3      709            723\n#&gt;  9  2013     1     1      557            600        -3      838            846\n#&gt; 10  2013     1     1      558            600        -2      753            745\n#&gt; # ℹ 336,766 more rows\n#&gt; # ℹ 14 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#&gt; #   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#&gt; #   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, gain &lt;dbl&gt;, hours &lt;dbl&gt;,\n#&gt; #   gain_per_hour &lt;dbl&gt;\n\nIf you only want to see the new columns, transmute() can be used.\n\nflights %&gt;% transmute(gain=arr_delay-dep_delay, hours=air_time/60, gain_per_hour=gain/hours)\n#&gt; # A tibble: 336,776 × 3\n#&gt;     gain hours gain_per_hour\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n#&gt;  1     9 3.78           2.38\n#&gt;  2    16 3.78           4.23\n#&gt;  3    31 2.67          11.6 \n#&gt;  4   -17 3.05          -5.57\n#&gt;  5   -19 1.93          -9.83\n#&gt;  6    16 2.5            6.4 \n#&gt;  7    24 2.63           9.11\n#&gt;  8   -11 0.883        -12.5 \n#&gt;  9    -5 2.33          -2.14\n#&gt; 10    10 2.3            4.35\n#&gt; # ℹ 336,766 more rows\n\nHere are an (incomplete) list of supported operators and functions.\n\nArithmetic operators: +, -, *, /, ^.\nModular arithmetic: %/% (integer division), %% (remainder).\nLogs: log(), log2(), log10().\nCumulative and rolling aggregates: cumsum(), cumprod(), cummin(), cummax(), cummean()\nLogical comparisons: &lt;, &lt;=, &gt;, &gt;=, !=.\n\n\n\n9.3.4 summarize() and group_by()\nsummarize() collapses a dataset to a single row. It computes values across all rows. It is usually paired with group_by(). Here are some examples.\n\nExample 9.3  \n\nflights %&gt;% group_by(year, month, day) %&gt;% \n    summarize(delay=mean(dep_delay, na.rm=TRUE))\n#&gt; `summarise()` has grouped output by 'year', 'month'. You can override using the\n#&gt; `.groups` argument.\n#&gt; # A tibble: 365 × 4\n#&gt; # Groups:   year, month [12]\n#&gt;     year month   day delay\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n#&gt;  1  2013     1     1 11.5 \n#&gt;  2  2013     1     2 13.9 \n#&gt;  3  2013     1     3 11.0 \n#&gt;  4  2013     1     4  8.95\n#&gt;  5  2013     1     5  5.73\n#&gt;  6  2013     1     6  7.15\n#&gt;  7  2013     1     7  5.42\n#&gt;  8  2013     1     8  2.55\n#&gt;  9  2013     1     9  2.28\n#&gt; 10  2013     1    10  2.84\n#&gt; # ℹ 355 more rows\n\n\n\nExample 9.4  \n\ndelays &lt;- flights %&gt;% \n    group_by(dest) %&gt;% \n    summarize(\n        count=n(), \n        dist=mean(distance, na.rm=TRUE),\n        delay=mean(arr_delay, na.rm=TRUE)\n    ) %&gt;% \n    filter(count&gt;20, dest!='HNL')\ndelays\n#&gt; # A tibble: 96 × 4\n#&gt;    dest  count  dist delay\n#&gt;    &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 ABQ     254 1826   4.38\n#&gt;  2 ACK     265  199   4.85\n#&gt;  3 ALB     439  143  14.4 \n#&gt;  4 ATL   17215  757. 11.3 \n#&gt;  5 AUS    2439 1514.  6.02\n#&gt;  6 AVL     275  584.  8.00\n#&gt;  7 BDL     443  116   7.05\n#&gt;  8 BGR     375  378   8.03\n#&gt;  9 BHM     297  866. 16.9 \n#&gt; 10 BNA    6333  758. 11.8 \n#&gt; # ℹ 86 more rows\n\n\ngroup_by() can also be used together with mutate() and filter().\n\nExample 9.5  \n\nflights %&gt;%\n    group_by(dest) %&gt;%\n    filter(n() &gt; 365) %&gt;%\n    filter(arr_delay &gt; 0) %&gt;%\n    mutate(prop_delay = arr_delay / sum(arr_delay)) %&gt;%\n    select(year:day, dest, arr_delay, prop_delay)\n#&gt; # A tibble: 131,106 × 6\n#&gt; # Groups:   dest [77]\n#&gt;     year month   day dest  arr_delay prop_delay\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1  2013     1     1 IAH          11  0.000111 \n#&gt;  2  2013     1     1 IAH          20  0.000201 \n#&gt;  3  2013     1     1 MIA          33  0.000235 \n#&gt;  4  2013     1     1 ORD          12  0.0000424\n#&gt;  5  2013     1     1 FLL          19  0.0000938\n#&gt;  6  2013     1     1 ORD           8  0.0000283\n#&gt;  7  2013     1     1 LAX           7  0.0000344\n#&gt;  8  2013     1     1 DFW          31  0.000282 \n#&gt;  9  2013     1     1 ATL          12  0.0000400\n#&gt; 10  2013     1     1 DTW          16  0.000116 \n#&gt; # ℹ 131,096 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe already use it in the above examples. This is to compute the number of observations in the current group. This function is implemented specifically for each data source and can only be used from within summarise(), mutate() and filter().",
    "crumbs": [
      "Part II: R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R for Data Sciences</span>"
    ]
  },
  {
    "objectID": "contents/9/intro.html#ggplot2",
    "href": "contents/9/intro.html#ggplot2",
    "title": "9  R for Data Sciences",
    "section": "9.4 ggplot2",
    "text": "9.4 ggplot2\nThis is the graphing package for R in tidyverse. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more faster by learning one system and applying it in many places. The main function is ggplot().\nggplot2 will be uploaded with tidyverse.\n\nlibrary(tidyverse)\n\nWe use the dataset mpg as the example. This dataset comes with ggplot2. Once you load ggplot2 you can directly find the dataset by typing mpg.\n\nmpg\n#&gt; # A tibble: 234 × 11\n#&gt;    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n#&gt;    &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt;  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n#&gt;  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n#&gt;  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n#&gt;  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n#&gt;  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n#&gt;  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n#&gt;  7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n#&gt;  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n#&gt;  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n#&gt; 10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n#&gt; # ℹ 224 more rows\n\nThe syntax for ggplot() is ::: {.cell}\nggplot(data = &lt;DATA&gt;) +\n    &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\n:::\nggplot(data=&lt;DATA&gt;) create a plot without any geometric elements. It simply creates the canvas paired with the dataset.\nThen you add one or more layers to ggplot() to complete the graph. The function &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) adds a layer to the plot. You may add as many layers as you want.\nEach geom function takes a mapping argument. This defines how variables in the dataset are mapped to visual properties. mapping is always paired with aes(x=, y=). This\nHere is a quick example.\n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy))\n\n\n\n\n\n\n\n\n\n9.4.1 Aesthetic mappings\nAn aesthetic is a visual property of the objects in your plot. It include things like the size, the shape or the color of the points. The variables set in aes() will change the aesthetic apperance of the geometric objects according to the variables. If the variables are set outside aes(), the apperance will be fixed. Please see the following examples.\nNote that the variables in aes() other than x and y will automatically get legends.\n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")\n\n\n\n\n\n\n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy, color = class))\n\n\n\n\n\n\n\n\n\n\n9.4.2 facet\nFor categorical data, you can split the plot into facets. This facet function will be attached as a layer followed by a + sign.\n\nTo facet your plot by a single variable, use facet_wrap(). The first argument should be a formula, which you create with ~ followed by a variable name.\nTo facet your plot by two variables, use facet_grid(). The first argument is a formula which contains two variable names separated by a ~.\n\n\nExample 9.6  \n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy)) +\n    facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n\n\nYou may look at the variables to see the relations with the plot.\n\nunique(mpg$class)\n#&gt; [1] \"compact\"    \"midsize\"    \"suv\"        \"2seater\"    \"minivan\"   \n#&gt; [6] \"pickup\"     \"subcompact\"\n\n\n\nExample 9.7  \n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy)) +\n    facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n\nYou may look at the variables to see the relations with the plot.\n\nunique(mpg$drv)\n#&gt; [1] \"f\" \"4\" \"r\"\nunique(mpg$cyl)\n#&gt; [1] 4 6 8 5\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe ~ symbol is used to define a formula. The formula is a R object, which provide the pattern of a “formula”. Therefore drv~cyl means that drv is a function of cyl.\n\n\n\n\n9.4.3 geom objects\nA geom is the geometrical object that a plot uses to represent data. When drawing plots, you just need to attach those geometrical objects to a ggplot canvas with + symbol. Some of the geometrical objects can automatically do statistical transformations. The statistical transformations is short for stat, and the stat argument in those geom will show which statistical transformations are applied.\n\ngeom_point() draws scatter plot.\ngeom_smooth() draws smooth line approximation.\ngeom_bar() draws bar plot.\ngeom_histogram() draws histogram.\n\nThe arguments can be written in ggplot(). All the later geom will get those arguments from ggplot(). If the arguments are written again in geom object, it will override the ggplot() arguments.\n\nExample 9.8  \n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy))\n\n\n\n\n\n\n\n\nggplot(data = mpg) +\n    geom_smooth(mapping = aes(x = displ, y = hwy), formula = y ~ x, method = \"loess\")\n\n\n\n\n\n\n\n\nggplot(data = mpg) +\n    geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv), formula = y ~ x, method = \"loess\")\n\n\n\n\n\n\n\n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy)) +\n    geom_smooth(mapping = aes(x = displ, y = hwy), formula = y ~ x, method = \"loess\")\n\n\n\n\n\n\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +\n    geom_point(mapping = aes(color = class)) +\n    geom_smooth(\n        data = filter(mpg, class == \"subcompact\"),\n        se = FALSE, \n        formula = y ~ x, method = \"loess\"\n        )",
    "crumbs": [
      "Part II: R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R for Data Sciences</span>"
    ]
  },
  {
    "objectID": "contents/9/intro.html#exercises",
    "href": "contents/9/intro.html#exercises",
    "title": "9  R for Data Sciences",
    "section": "9.5 Exercises",
    "text": "9.5 Exercises\n\nExercise 9.1 How can you tell if an object is a tibble?\n\n\nExercise 9.2 Compare and contrast the following operations on a data.frame and equivalent tibble. What is different? Why might the default data frame behaviors cause you frustration?\n\ndf &lt;- data.frame(abc = 1, xyz = \"a\")\ndf$x\ndf[, \"xyz\"]\ndf[, c(\"abc\", \"xyz\")]\n\n\n\nExercise 9.3 If you have the name of a variable stored in an object, e.g., var &lt;- \"xyz\", how can you extract the reference variable from a tibble? You may use the following codes to get a tibble.\n\ntbl &lt;- tibble(abc = 1, xyz = \"a\")\n\n\n\nExercise 9.4 Practice referring to nonsyntactic names in the following data.frame by:\n\nExtracting the variable called 1.\nCreating a new column called 3, which is 2 divided by 1.\nRenaming the columns to one, two, and three:\n\n\nannoying &lt;- tibble(\n`1` = 1:10,\n`2` = `1` * 2 + rnorm(length(`1`))\n)\n\n\n\nExercise 9.5 Both unite() and separate() have a remove argument. What does it do? Why would you set it to FALSE?\n\n\nExercise 9.6 Use flights dataset. Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.\n\n\n\n\n\nExercise 9.7 Please make the following data tidy.\n\nlibrary(tidyverse)\ndf &lt;- tibble(Day=1:5, `Plant_A_Height (cm)`=c(0.5, 0.7, 0.9, 1.3, 1.8), `Plant_B_Height (cm)`=c(0.7, 1, 1.5, 1.8, 2.2))\n\n\n\nExercise 9.8 Please use the flights dataset. Please find all flights that :\n\nHad an arrival delay of two or more hours.\nFlew to IAH or HOU.\nWere operated by United, American or Delta.\nDeparted in summer (July, August, and September).\nArrived more than two hours late, but didn’t leave late.\nWere delayed by at least an hour, but made up over 30 minutes in flight.\nDeparted between midnight and 6 a.m. (inclusive).\n\n\n\nExercise 9.9 Re-create the R code necessary to generate the following graphs. The dataset is mpg.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] Wickham, H. and Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media.",
    "crumbs": [
      "Part II: R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R for Data Sciences</span>"
    ]
  },
  {
    "objectID": "contents/10/intro.html#who-tb-dataset",
    "href": "contents/10/intro.html#who-tb-dataset",
    "title": "10  Projects with R",
    "section": "10.1 WHO TB dataset",
    "text": "10.1 WHO TB dataset\nLet us explore the tuberculosis cases data. The dataset is provided by WHO and can be downloaded from here. tidyr also provides the dataset. You may directly get the dataset after you load tidyr from tidyverse. The variable description can be found from tidyr documentations.\n\nlibrary(tidyverse)\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nwho\n#&gt; # A tibble: 7,240 × 60\n#&gt;    country  iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544\n#&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 Afghani… AF    AFG    1980          NA           NA           NA           NA\n#&gt;  2 Afghani… AF    AFG    1981          NA           NA           NA           NA\n#&gt;  3 Afghani… AF    AFG    1982          NA           NA           NA           NA\n#&gt;  4 Afghani… AF    AFG    1983          NA           NA           NA           NA\n#&gt;  5 Afghani… AF    AFG    1984          NA           NA           NA           NA\n#&gt;  6 Afghani… AF    AFG    1985          NA           NA           NA           NA\n#&gt;  7 Afghani… AF    AFG    1986          NA           NA           NA           NA\n#&gt;  8 Afghani… AF    AFG    1987          NA           NA           NA           NA\n#&gt;  9 Afghani… AF    AFG    1988          NA           NA           NA           NA\n#&gt; 10 Afghani… AF    AFG    1989          NA           NA           NA           NA\n#&gt; # ℹ 7,230 more rows\n#&gt; # ℹ 52 more variables: new_sp_m4554 &lt;dbl&gt;, new_sp_m5564 &lt;dbl&gt;,\n#&gt; #   new_sp_m65 &lt;dbl&gt;, new_sp_f014 &lt;dbl&gt;, new_sp_f1524 &lt;dbl&gt;,\n#&gt; #   new_sp_f2534 &lt;dbl&gt;, new_sp_f3544 &lt;dbl&gt;, new_sp_f4554 &lt;dbl&gt;,\n#&gt; #   new_sp_f5564 &lt;dbl&gt;, new_sp_f65 &lt;dbl&gt;, new_sn_m014 &lt;dbl&gt;,\n#&gt; #   new_sn_m1524 &lt;dbl&gt;, new_sn_m2534 &lt;dbl&gt;, new_sn_m3544 &lt;dbl&gt;,\n#&gt; #   new_sn_m4554 &lt;dbl&gt;, new_sn_m5564 &lt;dbl&gt;, new_sn_m65 &lt;dbl&gt;, …\n\nBased on the description of varaibles, we understand that\n\ncountry, iso2, iso3 are all refered to country names (and thus they are redundant).\nColumns after year, like new_sp_m014 etc., are counts of new TB cases recorded by groups. The code has three parts, most of which are separated by _ (but there are some exceptions).\n\nThe first part is always new.\nThe second part is a code for method of diagnosis:\n\nrel = relapse,\nsn = negative pulmonary smear,\nsp = positive pulmonary smear,\nep = extrapulmonary.\n\nThe third part is a code for gender (f = female, m = male) and a code for age group:\n\n014 = 0-14 yrs of age,\n1524 = 15-24 years of age,\n2534 = 25 to 34 years of age,\n3544 = 35 to 44 years of age,\n4554 = 45 to 54 years of age,\n5564 = 55 to 64 years of age,\n65 = 65 years of age or older\n\n\n\nTherefore to clean the data, we need the following steps.\n\nExample 10.1 Gather together all the columns from new_sp_m014 to newrel_f65.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nwholonger &lt;- who %&gt;% pivot_longer(cols=5:60, names_to='group', values_to='counts')\n\n\n\n\n\nThen we use stringr::str_replace() to replace newrel by new_rel.\n\nwholonger2 &lt;- wholonger %&gt;% mutate(key=str_replace(group, 'newrel', 'new_rel'))\n\n\nExample 10.2 Parse the column group into columns.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nwholonger3 &lt;- wholonger2 %&gt;% \n        separate(key, into=c('new', 'type', 'genderage'), sep='_') %&gt;% \n        separate(genderage, into=c('gender', 'age'), sep=1)\n\n\n\n\n\n\nExample 10.3 Pick the columns that matters.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntidywho &lt;- wholonger3[c('country', 'year', 'type', 'gender', 'age', 'counts')]\n\n\n\n\n\nWe could use the pipe symbol to connect all the above steps.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntidywho &lt;- who %&gt;% \n    pivot_longer(cols=5:60, names_to='group', values_to='counts') %&gt;% \n    mutate(key=str_replace(group, 'newrel', 'new_rel')) %&gt;% \n    separate(key, into=c('new', 'type', 'genderage'), sep='_') %&gt;% \n    separate(genderage, into=c('gender', 'age'), sep=1) %&gt;% \n    select('country', 'year', 'type', 'gender', 'age', 'counts')",
    "crumbs": [
      "Part II: R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projects with R</span>"
    ]
  },
  {
    "objectID": "contents/10/intro.html#us-babynames",
    "href": "contents/10/intro.html#us-babynames",
    "title": "10  Projects with R",
    "section": "10.2 US Babynames",
    "text": "10.2 US Babynames\nLet us use R to solve the babynames dataset again.\nThe first task is to read those files.\n\nExample 10.4 Please read files and put the data into one tibble. The dataset can be downloaded from here as a zip file.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\npath &lt;- 'assessts/datasets/babynames/yob'\ndfs &lt;- map(1880:2010, function(y){\n    filepath &lt;- paste0(path, as.character(y), '.txt')\n    df_individual &lt;- tibble(read.csv(filepath, header=FALSE))\n    names(df_individual) &lt;- c('name', 'gender', 'counts')\n    df_individual$year &lt;- y\n    df_individual\n})\ndf &lt;- bind_rows(dfs)\n\n\n\n\n\n\nExample 10.5 Please plot the total births by gender and year.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf %&gt;% \n    group_by(gender, year) %&gt;% \n    summarize(total_num=sum(counts)) %&gt;% \n    ggplot() +\n        geom_line(mapping = aes(x=year, y=total_num, color=gender))\n#&gt; `summarise()` has grouped output by 'gender'. You can override using the\n#&gt; `.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 10.6 Please compute the proportions of each name relateive to the total number of births per year per gender.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf %&gt;% \n    group_by(gender, year) %&gt;% \n    mutate(prop=counts/sum(counts))\n#&gt; # A tibble: 1,690,784 × 5\n#&gt; # Groups:   gender, year [262]\n#&gt;    name      gender counts  year   prop\n#&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n#&gt;  1 Mary      F        7065  1880 0.0776\n#&gt;  2 Anna      F        2604  1880 0.0286\n#&gt;  3 Emma      F        2003  1880 0.0220\n#&gt;  4 Elizabeth F        1939  1880 0.0213\n#&gt;  5 Minnie    F        1746  1880 0.0192\n#&gt;  6 Margaret  F        1578  1880 0.0173\n#&gt;  7 Ida       F        1472  1880 0.0162\n#&gt;  8 Alice     F        1414  1880 0.0155\n#&gt;  9 Bertha    F        1320  1880 0.0145\n#&gt; 10 Sarah     F        1288  1880 0.0142\n#&gt; # ℹ 1,690,774 more rows\n\n\n\n\n\n\nExample 10.7 We would like to keep the first 100 names (by counts) in each year and save it as a new tibble top100.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntop100 &lt;- df %&gt;% \n        group_by(gender, year) %&gt;% \n        top_n(100, wt=counts)\n\n\n\n\n\n\nExample 10.8 Please draw the trend of John, Harry, Mary in top100 by counts.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nnamelist &lt;- c('John', 'Harry', 'Mary')\ntop100 %&gt;% \n    filter(name %in% namelist) %&gt;% \n    ggplot() +\n        geom_line(mapping=aes(x=year, y=counts, color=name))\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 10.9 Now we would like to analyze the ending of names. Please get a tibble that contains the counts of ending letter per year per gender. We mainly focus on 1910, 1960 and 2010.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf %&gt;% \n    filter(year %in% c(1910, 1960, 2010)) %&gt;% \n    mutate(ending=str_sub(name, -1, -1), \n           year=as.factor(year)) %&gt;% \n    group_by(gender, year, ending) %&gt;% \n    summarise(ending_counts=sum(counts)) %&gt;% \n    ggplot() +\n        geom_col(\n            mapping = aes(\n                x=ending, \n                y=ending_counts, \n                fill=year,\n                ), \n            position = \"dodge\",\n        ) +\n        facet_wrap(~gender, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 10.10 Please draw the line plot to show the trending of certain letters through years. Here we choose d, n and y.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf %&gt;% \n    mutate(ending=str_sub(name, -1, -1)) %&gt;% \n    group_by(year, ending) %&gt;% \n    summarise(ending_counts=sum(counts)) %&gt;% \n    filter(ending %in% c('d', 'n', 'y')) %&gt;% \n    ggplot() +\n        geom_line(\n            mapping = aes(\n                x=year, \n                y=ending_counts, \n                color=ending\n            )\n        )",
    "crumbs": [
      "Part II: R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projects with R</span>"
    ]
  },
  {
    "objectID": "contents/10/intro.html#references",
    "href": "contents/10/intro.html#references",
    "title": "10  Projects with R",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Part II: R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projects with R</span>"
    ]
  },
  {
    "objectID": "contents/app/setup.html#sec-vscode",
    "href": "contents/app/setup.html#sec-vscode",
    "title": "Appendix A — Python Setup",
    "section": "A.1 VS Code + Anaconda",
    "text": "A.1 VS Code + Anaconda\nNote that all the following steps are tested in Windows 10/11. If you use other operation systems please contact me.\n\nGo to Miniconda download page. Download and install Miniconda, and set things up in the following way.\n\n\n\n\n\n\n\nMore details.\n\n\n\n\n\n\nMiniconda is Python + conda, without any unessential packages. We choose this version because it is fast: the download package is small and easy to install.\nAfter you finish installing Miniconda, you may find the Anaconda Prompt (miniconda 3) from the start menu. Click it to start using it.\nYou may use the following command to install packages that is used in this course. More on this will be discussed later.\n\nconda install pandas numpy matplotlib seaborn\n\nIf you feel conda is slow, you may change the conda solver to be libmamba using the following command. It may be faster than the classic conda solver.\n\nconda config --set solver libmamba\n\nOnce these packages are installed, you may close the command prompt window and proceed to the next step.\n\n\n\n\n\n\n\n\n\n\nAn alternative way.\n\n\n\n\n\nIf you have enough time and space, you may go to Anaconda download page, download and install Anaconda.\nAnaconda is Miniconda + tons of preinstalled packages + many other tools that may not be used in this course.\n\n\n\n\nGo to VS Code download page. Download and install VS Code. Actually Anaconda contains one copy of VS Code. Here I just assume that some of you intall VS Code before Anaconda.\nWhen installing VS Code, you may accept all default settings. When installing Anaconda, please pay attention to the PATH setting.\n\n\n\n\n\n\nThe first box is unchecked by default. This setting is related to the ability to easily run Python code in Terminals. I recommend you to check it. If you don’t check it during this step, you may add it to the system environment variable PATH manually later.\n\nThe UI of VS Code looks as follows.\n\n\n\n\n\n\nPlease look at the fifth tab from the left sidebar. It is the Extension tab.\n\n\n\n\n\nPlease search for python and install the first Python extension from Microsoft. It will actually install five extensions. These are all we need for now.\n\nAfter all are installed, go to the first Explorer tab on the left side bar, and Open Folder. This is the working directory for your project.\n\n\n\n\n\n\nChoose one folder and start a new .py file.\n\n\n\n\n\n\nIf everything is setup correctly, you may see the Python version and environment name at the right lower corner. In our case the environment name is base. We will need it in the future.\n\n\n\n\n\n\nNote that we are not looking at the Python for Language Mode. If you see Select Interpreter there, it means that VS Code doesn’t find your Python interpreter. Please restart VS Code or select it manually, or check whether Anaconda is installed correctly.\n\n\n\n\n\nTo check whether everything is setup correctly, please run the following tests.\n\nUse ctrl+shift+p to open the Command Palette, type “Jupyter: Create Interactive Window” and press enter to open the Jupyter interactive window.\n\n\n\n\n\n\nIf the interactive window starts and you see the loading infomation of your kernel as follows, especially you see the environment name on the right upper corner, then you get everything correctly. However we will still do more tests.\n\n\n\n\n\n\nIn the window type import numpy as np to test whether you are able to import packages. If you don’t see any error messages then it means good.\n\n\n\n\n\n\n\nIn the editor window, type import numpy as np and right click the body to choose Run Current File in Interactive Window, and see whether it runs in interactive window.\n\n\n\n\n\n\n\nOpen the terminal. Please use Command Prompt instead of Powershell. Activate the conda environment by type the command conda activate base in the example above. Please change the name to match your own environment. If conda cannot be recognized, please register Python and Anaconda to the system environment path. Please see the next Appendix for details.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python Setup</span>"
    ]
  },
  {
    "objectID": "contents/app/setup.html#sec-googlecolab",
    "href": "contents/app/setup.html#sec-googlecolab",
    "title": "Appendix A — Python Setup",
    "section": "A.2 Google Colab",
    "text": "A.2 Google Colab\nGoogle Colab is a product from Google Research, that allows anybody to write and execute arbitrary Python code through the browser, and is especially well suited to machine learning, data analysis and education.\nHere is the link to Google Colab. To use it you should have a Google account. Otherwise it is very simple to start, since a lot of packages for our course are already installed.\n\nA.2.1 Install packages\nIf you would like to install more packages, you can type the following code in a code cell and execute it.\n%pip install &lt;pkg name&gt;\n%conda install &lt;pkg name&gt;\nThe drawback here is that Google Colab can only stay for 24 hours. After that, all additionaly installed packages will be earsed. However you may put the installation code mentioned above at the beginning of your notebook and these packages will be installed every time you run the notebook.\n\n\nA.2.2 Upload files\nYou may directly upload files to the working directory of Google Colab. This has to be done in the browser. When working with these files, you may just use relative paths.\nThe drawback here is that Google Colab can only stay for 24 hours. After that, although your .ipynb files will be stores, all other files will be earsed.\n\n\nA.2.3 Mount Google Drive\nOne way to let the uploaded files stay in cloud is to upload them to Google Drive, and then load your Google Drive contents from Google Colab.\nGoole Drive is a cloud storage service provided by Google. When you register a Google account you will be automatically assigned a Google Drive account. You may get access to it from this link.\nHere are the steps to mount Google Drive:\n\nUpload your files to your Google Drive.\nRun the following codes in Colab code cells before you are loading the uploaded files:\n\n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n\n\nA window pop up asking you about the permission. Authorize and the drive is mounted.\nTo work in directories, the most popular commands are\n\n%ls: list all files and folders in the working directory.\n%cd + folder name: Get into a specific folder.\n%cd..: Get into the parent folder. Then use these commands to find the files your just uploaded.\n\nFinally you may directly get access to those files just like they are in the working directory.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python Setup</span>"
    ]
  },
  {
    "objectID": "contents/app/path.html",
    "href": "contents/app/path.html",
    "title": "Appendix B — Environemnt Variable PATH",
    "section": "",
    "text": "Here are the steps to edit the system environment variables in Windows 10/11.\n\nFirst in the start menu search for Edit the system environment variables.\n\n\n\n\n\n\n\nThen click the Environment Variables... button at the right lower corner.\n\n\n\n\n\n\n\nFind the Path variable in either the upper window or the lower window. Use which one depends on whether you want to register the variable for the user or for the machine. In this example I add for the user.\n\n\n\n\n\n\n\nFinally double click the variable and add the following path to it. You need to make changes according to your installation. I recommend you to locate your Anaconda installation first to get the path.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Environemnt Variable `PATH`</span>"
    ]
  },
  {
    "objectID": "contents/app/virtenv.html#package-management",
    "href": "contents/app/virtenv.html#package-management",
    "title": "Appendix C — Python Virtual Environment",
    "section": "C.1 Package management",
    "text": "C.1 Package management\nThere are two most popular package management tools for Python, pip and conda.\n\npip is the official package management tool for Python.\nconda was originally developed by Anaconda Inc., and later became open-sourced.\n\nIn this course, we mainly focus on conda since it is designed towards Data Science.\n\nInstall packages. You may specify the particular version number.\n\nconda install &lt;pkg name&gt;\nconda install &lt;pkg name&gt;=&lt;version number&gt;\n\nList all installed packages, or list several specific installed packages:\n\nconda list\nconda list &lt;pkg names&gt;\n\nUpdate packages.\n\nconda updata &lt;pkg name&gt;\n\nRemove packages.\n\nconda remove &lt;pkg name&gt;\n\nIf the packages you want is in PyPI but not in conda channels, you may use pip to install that package.\n\npip install &lt;pkg name&gt;\nNote that if the package is in both conda channels and PyPI, it is recommended not to mix pip and conda. If you start from conda, just stick to conda. Only use pip when you have to.\n\n\n\n\n\n\nPyPI and conda-forge\n\n\n\n\n\nThe package managemers will download packages from online repository to install in your environment. By default, pip and conda use two different repositories.\n\nPyPI: PyPI stands for Python Package Index. It is the official third-party software repository for Python. pip use it as the default source for packages and their dependencies. Packages on PyPI are typically uploaded by the author of the Python package.\nconda-forge: Anaconda, Inc. provides several channels that host packages. Conda-Forge is one of the most important channels. Although it is a community project, it is now the recommended channel to get packages through conda. In conda-forge, package maintainers can be different than the original author of the package.\n\nUsually PyPI contains more packages than conda-forge, and the versions of packages get to PyPI faster. However, when using conda through conda-forge, more safty checks are done and conda will try its best to make the installed packages compatible.\nTo install from conda-forge using conda, you should add an argument -c:\nconda install -c conda-forge &lt;pkg name&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Python Virtual Environment</span>"
    ]
  },
  {
    "objectID": "contents/app/virtenv.html#virtual-environments",
    "href": "contents/app/virtenv.html#virtual-environments",
    "title": "Appendix C — Python Virtual Environment",
    "section": "C.2 Virtual environments",
    "text": "C.2 Virtual environments\nVirtual environments provide a project-specific version of installed packages. This both helps you to faithfully reproduce your environment as well as isolate the use of packages so that upgrading a package in one project doesn’t break other projects. In this section we are going to use conda to manage environments. The main reference is the official document. We will just list the minimal working examples here. For more functions please read the official document.\n\nTo create an environment:\n\nconda create --name &lt;env name&gt;\n\nTo activate an environment:\n\nconda activate &lt;env name&gt;\nAfter you create a new environment and activate it, you may start to use the commands from the previous section to install packages.\n\nTo deactivate an environment:\n\nconda deactivate\n\nTo remove an environemnt, both of the following commands work:\n\nconda remove --name &lt;env name&gt; --all\nconda env remove -n &lt;env name&gt;\n\nTo get all enviroments in the system:\n\nconda env list\nconda info --envs\n\n\n\n\n\n\npip and venv\n\n\n\n\n\nUnlike conda, pip is only a package manager, and it doesn’t provide any virtual environment functions. The default virtual environment tool for Python is venv. You may go to the official document for more infomation about venv.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Python Virtual Environment</span>"
    ]
  },
  {
    "objectID": "contents/app/virtenv.html#building-identical-environments",
    "href": "contents/app/virtenv.html#building-identical-environments",
    "title": "Appendix C — Python Virtual Environment",
    "section": "C.3 Building identical environments",
    "text": "C.3 Building identical environments\nSometimes people want to build identical environments. This is done by recording the versions of all packages in the current environment into a spec list file. When rebuilding Python will pull the spec list file out and install the packages of the speicific versions based on the list.\nIn the process, there are two steps. First generate the pacakge spec list file. Second create an environment based on the list. Note that the spec list file generated by different methods have different formats. Therefore you have to use the correct command to restore the environment.\n\nC.3.1 Using conda\nTo produce a spec list, use the following command:\nconda list --explicit &gt; spec-file.txt\nTo install the packages, use the following command:\nconda create --name &lt;env name&gt; --file spec-file.txt\nA typical conda environment file looks like the following example.\n# This file may be used to create an environment using:\n# $ conda create --name &lt;env&gt; --file &lt;this file&gt;\n# platform: win-64\n@EXPLICIT\nhttps://repo.anaconda.com/pkgs/main/win-64/conda-env-2.6.0-1.conda\nhttps://repo.anaconda.com/pkgs/r/win-64/_r-mutex-1.0.0-anacondar_1.conda\nhttps://repo.anaconda.com/pkgs/main/win-64/blas-1.0-mkl.conda\n\n\nC.3.2 Using pip freeze\nThe classic popular way is pip freeze. To produce a spec list, use the following command:\npip freeze &gt; requirements.txt\nTo install the packages, use the following command:\npip install -r requirements.txt\nA typical pip requirements.txt file looks like the following example.\nanyio @ file:///C:/ci/anyio_1644481856696/work/dist\nargon2-cffi @ file:///opt/conda/conda-bld/argon2-cffi_1645000214183/work\nargon2-cffi-bindings @ file:///C:/ci/argon2-cffi-bindings_1644569876605/work\nasttokens==2.0.7\nattrs @ file:///C:/b/abs_09s3y775ra/croot/attrs_1668696195628/work\nBabel @ file:///tmp/build/80754af9/babel_1620871417480/work\nbackcall==0.2.0\n\n\n\n\n\n\nNote\n\n\n\nActually both ways are NOT satisfying. Therefore there are a lot of new tools coming out to deal with this task, like pipreqs and poetry. Here for simplicity we just briefly introduce the most basic ones.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Python Virtual Environment</span>"
    ]
  },
  {
    "objectID": "contents/app/virtenv.html#mamba",
    "href": "contents/app/virtenv.html#mamba",
    "title": "Appendix C — Python Virtual Environment",
    "section": "C.4 mamba",
    "text": "C.4 mamba\nmamba is a reimplementation of the conda package manager in C++. It can be installed directly from conda-forge.\nconda install -c conda-forge mamba\nAfter you install it in one of your environment, you may use it in all your environments.\nmamba can be treated as a drop-in replacement of conda. All commands we mentioned above can be rewritten by replacing conda with mamba. One of the reasons to use mamba over conda is that mamba runs so much faster than conda.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Python Virtual Environment</span>"
    ]
  },
  {
    "objectID": "contents/app/concepts.html#language",
    "href": "contents/app/concepts.html#language",
    "title": "Appendix D — Some Hard Python concepts",
    "section": "D.1 Language",
    "text": "D.1 Language\nPython is a programming language. In other words, it is a collection of syntaxes.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Some Hard Python concepts</span>"
    ]
  },
  {
    "objectID": "contents/app/concepts.html#interpreters",
    "href": "contents/app/concepts.html#interpreters",
    "title": "Appendix D — Some Hard Python concepts",
    "section": "D.2 Interpreters",
    "text": "D.2 Interpreters\nPython needs to be interpreted into codes that computers can understand. Therefore there should be some programs that translate Python scripts. These programs are called interpreters.\nCPython is the refernce interpreter of Python programming language, and it is the most widely used ones for Python. It is written in C and Python. When Python programming language introduces new features, they are developed based on CPython, and are first implemented in CPython. Sometimes an interpreter is also called an implementation.\nThere are alternatives to CPython, like PyPy, Jython, IronPython, etc.. In theory, any Python scripts should be able to run on any of these implementations, and the result should be the same. The differences mainly come from perfamance and compatiblity with non-Python packages. For example, CPython is executed by a C interpreter. Therefore it is very easy to write C-extensions for your Python code. Jython, since it is implemented in Java, makes it very easy to work with other Java programs that you can import any Java classes with no additional effort.\nSince CPython is most-widely used and tested, it is the best choice, at least for beginners. And actually, if you have no idea about this topic, but you use Python, it is highly possible that you are using CPython.\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe mentioned “interpreter” here. There are mainly two types of implimentations of programming langauges: interpreters and compilers. There are also some additional types like just-in-time compilers which can be treated as combinations of the two.\nPython is usually treated as an interpreted language since CPython is an interpreter. One of Python’s most useful features is its interactive interpreter, which allows for very fast testing of ideas without the overhead of creating test files as is typical in most programming languages.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Some Hard Python concepts</span>"
    ]
  },
  {
    "objectID": "contents/app/concepts.html#repl",
    "href": "contents/app/concepts.html#repl",
    "title": "Appendix D — Some Hard Python concepts",
    "section": "D.3 REPL",
    "text": "D.3 REPL\nThere are two ways to use Python interpreter. The default way is that Python interpreter reads a file and execute a script from there. The second way is called the intereactive shell, that Python interpreter read the input from user directly, and print the result immediately. The model is like code example: prompt the user for some code, and when they’ve entered it, execute it in the same process. This model is often called a REPL, or Read-Eval-Print-Loop.\nShell, terminal, console have different meanings in their original contexts. However, nowadays, especially when talking about Python intereactive shell, these terminologies are used interchangeably. They are referred to the frontend of the system. In other words, the main task for the Python intereactive shell is to handle the user inputs and communicate with the backend, which is also called a kernel. We won’t distinguish the real differences between these terminologies. The kernel will be discussed in the next section.\nThe standard interactive Python interpreter can be invoked on the command line with the python command. Note that you should make sure that the PATH system enviroment variable is configured, otherwise you have to specifiy the path to the Python execuatable file. To quit the intereactive shell you can type the commands quit()/exit()/Ctrl+Z then Enter.\n\n\n\n\n\n\nNote\n\n\n\nIn the REPL model, the backend (evaluation) is basically handled by the Python interpreter. The frontend is dealing with the user interface. Some typically tasks include the primary/secondary prompt and multi-line commands. The original REPL is very limited.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Some Hard Python concepts</span>"
    ]
  },
  {
    "objectID": "contents/app/concepts.html#ipython",
    "href": "contents/app/concepts.html#ipython",
    "title": "Appendix D — Some Hard Python concepts",
    "section": "D.4 IPython",
    "text": "D.4 IPython\nIPython was initially designed as an Enhanced interactive Python shell. However after many year’s development, the whole IPython project becomes too big to maintain as one single project. Therefore it is now split into many smaller projects. The two most popular projects are IPython and Jupyter. This is called the Big Split.\nThe current IPython play two fundamental roles:\n\nTerminal IPython as the familiar REPL;\nThe IPython kernel (which is defined below) that provides computation and communication with the frontend interfaces, like the notebook.\n\nThe core idea in the design of IPython is to abstract and extend the notion of a traditional REPL environment by decoupling the evaluation into its own process. We call this process a kernel: it receives execution instructions from clients and communicates the results back to them.\nThis decoupling allows us to have several clients connected to the same kernel, and even allows clients and kernels to live on different machines. This two-process model is now used by most of the Jupyter project.\nYou can launch the IPython shell on the command line with the ipython command (which similar to python case requires PATH configuration), and quit the shell with exit/exit()/quit/quit() commands.\nThe reference Python kernel provided by IPython is called ipykernel. With ipykernel you may create and maintain multiple kernels.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Some Hard Python concepts</span>"
    ]
  },
  {
    "objectID": "contents/app/concepts.html#jupyter",
    "href": "contents/app/concepts.html#jupyter",
    "title": "Appendix D — Some Hard Python concepts",
    "section": "D.5 Jupyter",
    "text": "D.5 Jupyter\nJupyter projects contain many subprojects, which includes Jupyter User Interfaces. The Jupyter user interfaces offer a foundation of interactive computing environments where scientific computing, data science, and analytics can be performed using a wide range of programming languages. This includes Jupyter console, Jupyter qtconsole, and Jupyter notebook. Here we mainly focus on Jupyter notebook.\nJupyter notebooks are structured data that represent your code, metadata, content, and outputs. When saved to disk, the notebook uses the extension .ipynb, and uses a JSON structure. After receiving the user input, the notebook communicates with the kernel using JSON messages sent over ZeroMQ sockets. The protocol used between the frontends and the kernel is described in Messaging in Jupyter.\nA kernel process can be connected to more than one frontend simultaneously. In this case, the different frontends will have access to the same variables.\nThis design was intended to allow easy development of different frontends based on the same kernel, but it also made it possible to support new languages in the same frontends, by developing kernels in those languages. The Jupyter Notebook Application has three main kernels: the ipykernel, irkernel and ijulia kernels. Actually the name of Jupyter comes from these three programming languages for data science: Julia, Python and R.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Some Hard Python concepts</span>"
    ]
  },
  {
    "objectID": "contents/app/concepts.html#multi-kernels-setup",
    "href": "contents/app/concepts.html#multi-kernels-setup",
    "title": "Appendix D — Some Hard Python concepts",
    "section": "D.6 Multi-kernels setup",
    "text": "D.6 Multi-kernels setup\nThis section is mainly following the official document.\nTo install one IPython kernel, you may use conda or pip to install ipykernel in the environment. If you want to have multiple IPython kernels for different virtualenvs or conda environments, you will need to specify unique names for the kernelspecs.\n\nActivate the environment you want.\n\nconda activate myenv\n\nInstall the kernel in the environment.\n\nconda install jupyter\npython -m ipykernel install --user --name myenv --display-name \"Python (myenv)\"\n--user means that the kernel is installed in the user’s folder instead of a system folder, and it can be removed. The --name value (in this case it is myenv) is used by Jupyter internally. These commands will overwrite any existing kernel with the same name. --display-name is what you see in the notebook menus.\n\nYou could use the command to find all kernels installed in your system.\n\njupyter kernelspec list\nAvailable kernels are shown, as well as the path to the kernel configuration file kernel.json. The most important configuration is the path to the Python interpreter executatable file.\n\n\nkernel.json\n\n{\n \"argv\": [\n  \"C:\\\\Users\\\\Xinli\\\\anaconda3\\\\envs\\\\myenv\\\\python.exe\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"Python (3.10)\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n }\n}\n\n\nThere is a possibility that Jupyter cannot find the kernel you create in a conda environment. In this case you may want to try nb_conda_kernels. This is a tool to enable a Jupyter notebook in one conda environment to access kernels found in other environments. It should be installed in the environment from which you run Jupyter Notebook or JupyterLab. This might be your base conda environment, but it need not be. After you finish installation, you may use jupyter kernelspec list to check whether it works.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Some Hard Python concepts</span>"
    ]
  },
  {
    "objectID": "contents/app/r.html#rstudio",
    "href": "contents/app/r.html#rstudio",
    "title": "Appendix E — R Setup",
    "section": "E.1 RStudio",
    "text": "E.1 RStudio\nFor R, the almost definite choice of IDE is RStudio. You may download and install it from the homepage. The installation and beginning usage is straightforward. RStudio is using the default R console.\nAlthough the best R IDE is RStudio, there are still some people willing to try other options. In the rest part of this section I will briefly describe how to set up R environment using other IDES/platforms.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>R Setup</span>"
    ]
  },
  {
    "objectID": "contents/app/r.html#radian",
    "href": "contents/app/r.html#radian",
    "title": "Appendix E — R Setup",
    "section": "E.2 radian",
    "text": "E.2 radian\nradian is an alternative console for the R program with multiline editing and rich syntax highlight. One would consider radian as a IPython clone for R, though its design is more aligned to Julia.\nradian is a R console. So to install it you should have an installation of R (version 3.4.0 or above). radian is mainly written in Python. Therefore to install it you should have an installation of Python (version 3.6 or above). Then you may use the following command to install radian.\nconda install -c conda-forge radian\nor\npip install -U radian\n\nE.2.1 Use the console\nFirst find the path to the console. If you use conda to install radian, the path is &lt;anaconda path&gt;/Scripts/radian.exe. When you directly run the executable file radian.exe, the console will be activated. You may start to run R code.\n\nYou may use q() to exit radian.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>R Setup</span>"
    ]
  },
  {
    "objectID": "contents/app/r.html#vs-code-configuration",
    "href": "contents/app/r.html#vs-code-configuration",
    "title": "Appendix E — R Setup",
    "section": "E.3 VS Code Configuration",
    "text": "E.3 VS Code Configuration\nThe main reference is the official document.\nIf you would like to use VS Code as your main IDE for R, it is usually recommended to install the following components.\n\nR extension for VS Code by REditorSupport. You may get it through this link, or search for it within VS Code.\nInstall languageserver in R.\n\n\ninstall.packages(\"languageserver\")\n\n\nradian. This is an alternative R console introduced above. Note that to use radian in VS Code you need to set the rterm variable in VS Code setting to be the path of your radian installation.\nhttpgd. This is a R package which is for better plotting in VS Code.\n\n\ninstall.packages(\"httpgd\")\n\nRunning R code is simply sending code to the R terminal. Before running R code, you could create an R terminal via command R: Create R terminal in the Command Palette. If you set radian path, this R terminal will be radian.\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou may adding the following Keyboard Shortcuts to VS Code. These are the same setting used in RStudio which are commonly used. The first two are using alt+- to produce &lt;- and the latter two are using ctrl+shift+m to produce %&gt;%.\n{\n\"key\": \"alt+-\",\n\"command\": \"type\",\n\"when\": \"editorTextFocus && editorLangId =~ /r|rmd|qmd/\",\n\"args\": {\"text\": \" &lt;- \"}\n},\n{\n\"key\": \"alt+-\",\n\"command\": \"workbench.action.terminal.sendSequence\",\n\"when\": \"terminalFocus\",\n\"args\": {\"text\": \" &lt;- \"}\n},\n{\n\"key\": \"ctrl+shift+m\",\n\"command\": \"type\",\n\"when\": \"editorTextFocus && editorLangId =~ /r|rmd|quarto/\",\n\"args\": {\"text\": \" %&gt;% \"}\n},\n{\n\"key\": \"ctrl+shift+m\",\n\"command\": \"workbench.action.terminal.sendSequence\",\n\"when\": \"terminalFocus\",\n\"args\": {\"text\": \" %&gt;% \"}\n},",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>R Setup</span>"
    ]
  },
  {
    "objectID": "contents/app/r.html#cloud-ide-options",
    "href": "contents/app/r.html#cloud-ide-options",
    "title": "Appendix E — R Setup",
    "section": "E.4 Cloud IDE options",
    "text": "E.4 Cloud IDE options\nThere are tons of online IDEs that supports R. The following two are the biggest names.\n\nE.4.1 Posit Cloud (formally RStudio Cloud)\nYou may directly go to the homepage to use RStudio from cloud. If you don’t use it a lot it should be free.\n\n\nE.4.2 Google Colab\nYou may use R in Google Colab. Note that by default R is disabled. You have to use a specific version of Google Colab through colab.to/r. After you get into the system, you may go to Edit-&gt;Notebook settings to change Runtime type to be R.\nThe rest is similar to Jupyter notebook, while the codes are now R codes.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>R Setup</span>"
    ]
  },
  {
    "objectID": "contents/app/re.html",
    "href": "contents/app/re.html",
    "title": "Appendix F — Regular expression",
    "section": "",
    "text": "Regular expressions provide a flexible way to search or match string patterns in text. A single expression, commonly called a regex, is a string formed according to the regular expression language. Python’s built-in re module is responsible for applying regular expressions to strings.\nFor details of the regular expression language in Python, please read the official documents from here. There are also many great websites for learning regex. This is one example.\nWe will briefly mentioned a few rules here.\n\n.: matches any character except a newline.\n\\d: matches any digit. It is the same as [0-9].\n\\D: matches any characters that are NOT \\d. It is the same as [^0-9].\n\\w: matches any alphabatic or numeric character. It is the same as [a-zA-Z0-9_].\n\\W: matches any characters that are NOT \\w.\n\\s: matches any whitespaces. It is the same as [\\t\\n\\r\\f\\v].\n\\S: mathces any characters that are not \\s.\n\\A: matches the start of the string.\n\\Z: matches the end of the string.\n*: Causes the resulting RE to match 0 or more repetitions of the preceding RE, as many repetitions as are possible.\n+: Causes the resulting RE to match 1 or more repetitions of the preceding RE, as many repetitions as are possible.\n?: Causes the resulting RE to match 0 or 1 repetitions of the preceding RE.\n*?, +?, ??: The *, +, and ? qualifiers are all greedy; they match as much text as possible. Adding ? after the qualifier makes it perform the match in non-greedy or minimal fashion; as few characters as possible will be matched.\n{m}: Specifies that exactly m copies of the previous RE should be matched.\n{m,n}: Causes the resulting RE to match from m to n repetitions of the preceding RE, attempting to match as many repetitions as possible.\n{m,n}?: Causes the resulting RE to match from m to n repetitions of the preceding RE, attempting to match as few repetitions as possible.\n[]: Used to indicate a set of characters.\n(): set groups.\n\n\n\n\n\n\n\nNote\n\n\n\nTo search multiple characters simutanously, you may use []. For example, [abc] means either a or b or c. However, [] doesn’t recognize special characters, so [\\s|\\w] means either \\ or s or \\ or w, instead of the pattern \\s or \\w.\nTo search such a pattern, you may use (|). For example, (\\s|\\w) means either \\s or \\w satisfies the pattern.\n\n\n\nExample F.1  \n\nimport re\ntext = \"foo bar\\t baz \\tqux\"\npattern = '\\s+'\nregex = re.compile(pattern)\nregex.split(text)\n\n['foo', 'bar', 'baz', 'qux']\n\n\n\n\n.match()\n.search()\n.findall()\n.split()\n.sub()\n\nWe can use () to specify groups, and use .groups() to get access to the results.\n\nExample F.2  \n\nimport re\npattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})'\nregex = re.compile(pattern, flags=re.IGNORECASE)\nm = regex.match('wesm@bright.net')\nm.groups()\n\n('wesm', 'bright', 'net')\n\n\n\nTo use regex to DataFrame and Series, you may directly apply .match, .findall, .replace after .str, with the regex pattern as one of the arguments.\n.extract is a method that is not from re. It is used to extract the matched groups and make them as a DataFrame.\n\nExample F.3  \n\nimport pandas as pd\nimport numpy as np\nmnames = ['movie_id', 'title', 'genres']\nmovies = pd.read_table('assests/datasets/movies.dat', sep='::',\n                       header=None, names=mnames, engine=\"python\",\n                       encoding='ISO-8859-1')\n\npattern = r'([a-zA-Z0-9_\\s,.?:;\\']+)\\((\\d{4})\\)'\nmovies = movies.join(movies.title.str.extract(pattern).rename(columns={0: 'movie title', 1: 'year'}))\n\n\n\n\nExercise F.1 (Regular expressions) Please use regular expressions to finish the following tasks.\n\nMatch a string that has an a followed by zero or more b’s.\nMatch a string that has an a followed by one or more b’s.\nMatch a string that has an a followed by zero or one b.\nMatch a string that has an a followed by three b’s.\n\n\n\nExercise F.2 (More regex) Find all words starting with a or e in a given string:\n\ntext = \"The following example creates an ArrayList with a capacity of 50 elements. Four elements are then added to the ArrayList and the ArrayList is trimmed accordingly.\"\n\n\n\nExercise F.3 (More regex) Write a Python code to extract year, month and date from a url1:\n\nurl1= \"https://www.washingtonpost.com/news/football-insider/wp/2016/09/02/odell-beckhams-fame-rests-on-one-stupid-little-ball-josh-norman-tells-author/\"\n\n\n\nExercise F.4 (More regex) Please use regex to parse the following str to create a dictionary.\n\ntext = r'''\n{\n    name: Firstname Lastname;\n    age: 100;\n    salary: 10000 \n}\n'''\n\n\n\nExercise F.5 Consider the following DataFrame.\n\ndata = [['Evert van Dijk', 'Carmine-pink, salmon-pink streaks, stripes, flecks.  Warm pink, clear carmine pink, rose pink shaded salmon.  Mild fragrance.  Large, very double, in small clusters, high-centered bloom form.  Blooms in flushes throughout the season.'],\n        ['Every Good Gift', 'Red.  Flowers velvety red.  Moderate fragrance.  Average diameter 4\".  Medium-large, full (26-40 petals), borne mostly solitary bloom form.  Blooms in flushes throughout the season.'], \n        ['Evghenya', 'Orange-pink.  75 petals.  Large, very double bloom form.  Blooms in flushes throughout the season.'], \n        ['Evita', 'White or white blend.  None to mild fragrance.  35 petals.  Large, full (26-40 petals), high-centered bloom form.  Blooms in flushes throughout the season.'],\n        ['Evrathin', 'Light pink. [Deep pink.]  Outer petals white. Expand rarely.  Mild fragrance.  35 to 40 petals.  Average diameter 2.5\".  Medium, double (17-25 petals), full (26-40 petals), cluster-flowered, in small clusters bloom form.  Prolific, once-blooming spring or summer.  Glandular sepals, leafy sepals, long sepals buds.'],\n        ['Evita 2', 'White, blush shading.  Mild, wild rose fragrance.  20 to 25 petals.  Average diameter 1.25\".  Small, very double, cluster-flowered bloom form.  Blooms in flushes throughout the season.']]\n  \ndf = pd.DataFrame(data, columns = ['NAME', 'BLOOM']) \ndf \n\n\n\n\n\n\n\n\nNAME\nBLOOM\n\n\n\n\n0\nEvert van Dijk\nCarmine-pink, salmon-pink streaks, stripes, fl...\n\n\n1\nEvery Good Gift\nRed. Flowers velvety red. Moderate fragrance...\n\n\n2\nEvghenya\nOrange-pink. 75 petals. Large, very double b...\n\n\n3\nEvita\nWhite or white blend. None to mild fragrance....\n\n\n4\nEvrathin\nLight pink. [Deep pink.] Outer petals white. ...\n\n\n5\nEvita 2\nWhite, blush shading. Mild, wild rose fragran...\n\n\n\n\n\n\n\nPlease use regex methods to find all the () in each columns.\n\n\nExercise F.6 From ser = pd.Series(['Apple', 'Orange', 'Plan', 'Python', 'Money']), find the words that contain at least 2 vowels.\n\n\nExercise F.7 Please download the given file with sample emails, and use the following code to load the file and save it to a string content.\n\nwith open('assests/datasets/test_emails.txt', 'r') as f:\n    content = f.read()\n\nPlease use regex to play with content.\n\nGet all valid email address in content, from both the header part or the body part.\nThere are two emails in content. Please get the sender’s email and the receiver’s email from content.\nPlease get the sender’s name.\nPlease get the subject of each email.\n\n\n\nExercise F.8 Extract the valid emails from the series emails. The regex pattern for valid emails is provided as reference.\n\nimport pandas as pd\nemails = pd.Series(['buying books at amazom.com',\n                    'rameses@egypt.com',\n                    'matt@t.co',\n                    'narendra@modi.com'])\npattern = '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,4}'",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Regular expression</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#example-1-usa.gov-data-from-bitly",
    "href": "contents/6/intro.html#example-1-usa.gov-data-from-bitly",
    "title": "6  Projects with Python",
    "section": "6.1 Example 1: USA.gov Data From Bitly",
    "text": "6.1 Example 1: USA.gov Data From Bitly\nIn 2011, URL shortening service Bitly partnered with the US government website USA.gov to provide a feed of anonymous data gathered from users who shorten links ending with .gov or .mil. The data is gotten from [1].\nThe data file can be downloaded from here. The file is mostly in JSON. It can be converted into a DataFrame by the following code.\n\nimport pandas as pd\nimport numpy as np\nimport json\npath = 'assests/datasets/example.txt'\ndf = pd.DataFrame([json.loads(line) for line in open(path)])\n\nWe mainly use tz and a columns. So let us clean it.\n\ndf['tz'] = df['tz'].fillna('Missing')\ndf['tz'][df['tz'] == ''] = 'Unknown'\ndf['a'] = df['a'].fillna('Missing')\ndf['a'][df['a'] == ''] = 'Unknown'\n\n\nExample 6.1 We first want to extract the timezone infomation from it. The timezone info is in the column tz. Please count different values in the columns tz.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntzone = df['tz']\ntvc = tzone.value_counts()\ntvc\n\ntz\nAmerica/New_York        1251\nUnknown                  521\nAmerica/Chicago          400\nAmerica/Los_Angeles      382\nAmerica/Denver           191\n                        ... \nEurope/Uzhgorod            1\nAustralia/Queensland       1\nEurope/Sofia               1\nAmerica/Costa_Rica         1\nAmerica/Tegucigalpa        1\nName: count, Length: 98, dtype: int64\n\n\n\n\n\n\nExample 6.2 We would like to visulize the value counts. You may just show the top ten results.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nimport seaborn as sns\nsns.barplot(x=tvc[:10].values, y=tvc[:10].index)\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6.3 We then would like to extract information from the column a. This column is about the agent of the connection. The important info is the part before the space ' '. Please get that part out and count values.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nagent = df['a']\nagent = agent.str.split(' ').str[0]\navc = agent.value_counts()\navc.head()\n\na\nMozilla/5.0               2594\nMozilla/4.0                601\nGoogleMaps/RochesterNY     121\nMissing                    120\nOpera/9.80                  34\nName: count, dtype: int64\n\n\n\n\n\n\nExample 6.4 Now let us assume that, if Windows appears in column a the user is using Windows os, if not then not. Please detect os, and classify it as Windows and Not Windows.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf['os'] = np.where(df['a'].str.contains('Windows'), 'Windows', 'Not Windows')\n\n\n\n\nNow make a bar plot about the counts based on os and timezone.\n\nExample 6.5 We first group the data by os and tz.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntz_os_counts = df.groupby(['tz', 'os']).size().unstack().fillna(0)\ntz_os_counts.head()\n\n\n\n\n\n\n\nos\nNot Windows\nWindows\n\n\ntz\n\n\n\n\n\n\nAfrica/Cairo\n0.0\n3.0\n\n\nAfrica/Casablanca\n0.0\n1.0\n\n\nAfrica/Ceuta\n0.0\n2.0\n\n\nAfrica/Johannesburg\n0.0\n1.0\n\n\nAfrica/Lusaka\n0.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\nExample 6.6 We then turn it into a DataFrame. You may use any methods.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nWe use the .stack(), .unstack() tricks here.\n\ntovc = tz_os_counts.stack()[tz_os_counts.sum(axis=1).nlargest(10).index]\ntovc.name = 'count'\ndftovc = pd.DataFrame(tovc).reset_index()\ndftovc.head()\n\n\n\n\n\n\n\n\ntz\nos\ncount\n\n\n\n\n0\nAmerica/New_York\nNot Windows\n339.0\n\n\n1\nAmerica/New_York\nWindows\n912.0\n\n\n2\nUnknown\nNot Windows\n245.0\n\n\n3\nUnknown\nWindows\n276.0\n\n\n4\nAmerica/Chicago\nNot Windows\n115.0\n\n\n\n\n\n\n\n\n\n\n\nExample 6.7 We may now draw the bar plot showing tz and os.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nsns.barplot(x='count', y='tz', hue='os', data=dftovc)",
    "crumbs": [
      "Part I: Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Projects with Python</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#example-2-us-baby-names-18802010",
    "href": "contents/6/intro.html#example-2-us-baby-names-18802010",
    "title": "6  Projects with Python",
    "section": "6.2 Example 2: US Baby Names 1880–2010",
    "text": "6.2 Example 2: US Baby Names 1880–2010\nThe United States Social Security Administration (SSA) has made available data on the frequency of baby names from 1880 through the present. Hadley Wickham, an author of several popular R packages, has often made use of this dataset in illustrating data manipulation in R. The dataset can be downloaded from here as a zip file. Please unzip it and put it in your working folder.\n\nExample 6.8 In the folder there are 131 .txt files. The naming scheme is yob + the year. Each file contains 3 columns: name, gender, and counts. We would like to add a column year, and combine all files into a single DataFrame. In our example, the year is from 1880 to 2010.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nimport pandas as pd\n\npath = 'assests/datasets/babynames/'\ndflist = list()\nfor year in range(1880, 2011):\n    filename = path + 'yob' + str(year) + '.txt'\n    df = pd.read_csv(filename, names=['name', 'gender', 'counts'])\n    df['year'] = year\n    dflist.append(df)\ndf = pd.concat(dflist, ignore_index=True)\n\n\n\n\n\nExample 6.9 Plot the total births by sex and year.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.relplot(data=df.groupby(['gender', 'year']).sum().reset_index(),\n            x='year', y='counts', hue='gender', kind='line')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6.10 For further analysis, we would like to compute the proportions of each name relative to the total number of births per year per gender.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndef add_prop(group):\n    group['prop'] = group.counts / group.counts.sum()\n    return group\n\ndf = df.groupby(['gender', 'year']).apply(add_prop)\ndf.head()\n\n\n\n\n\n\n\n\n\n\nname\ngender\ncounts\nyear\nprop\n\n\n\n\n0\nMary\nF\n7065\n1880\n0.077643\n\n\n1\nAnna\nF\n2604\n1880\n0.028618\n\n\n2\nEmma\nF\n2003\n1880\n0.022013\n\n\n3\nElizabeth\nF\n1939\n1880\n0.021309\n\n\n4\nMinnie\nF\n1746\n1880\n0.019188\n\n\n\n\n\n\n\n\n\n\n\nExample 6.11 Now we would like to keep the first 100 names in each year, and save it as a new DataFrame top100.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntop100 = (\n    df.groupby(['year', 'gender'])\n    .apply(lambda x: df.loc[x['counts'].nlargest(100).index])\n    .drop(columns=['year', 'gender'])\n    .reset_index()\n    .drop(columns='level_2')\n)\ntop100.head()\n\n\n\n\n\n\n\n\n\n\nyear\ngender\nname\ncounts\nprop\n\n\n\n\n0\n1880\nF\nMary\n7065\n0.077643\n\n\n1\n1880\nF\nAnna\n2604\n0.028618\n\n\n2\n1880\nF\nEmma\n2003\n0.022013\n\n\n3\n1880\nF\nElizabeth\n1939\n0.021309\n\n\n4\n1880\nF\nMinnie\n1746\n0.019188\n\n\n\n\n\n\n\nNote that level_2 is related to the original index after reset_index(). That’s why we don’t need it here.\n\n\n\n\nExample 6.12 Now we would like to draw the trend of some names: John, Harry and Mary.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nnamelist = ['John', 'Harry', 'Mary']\nsns.relplot(data=top100[top100['name'].isin(namelist)],\n            x='year', y='counts', hue='name', kind='line')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6.13 Please analyze the ending of names.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf['ending'] = df['name'].str[-1]\nendingcount = df.groupby(['gender', 'year', 'ending']).sum().reset_index()\n\n\n\n\n\nExample 6.14 We would like to draw barplots to show the distributions in year 1910, 1960 and 2010.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ncertainyear = endingcount[endingcount['year'].isin([1910, 1960, 2010])]\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(2, 1, figsize=(10,7))\nsns.barplot(data=certainyear[endingcount['gender']=='M'],\n            x='ending', y='prop', hue='year', ax=axs[0])\nsns.barplot(data=certainyear[endingcount['gender']=='F'],\n            x='ending', y='prop', hue='year', ax=axs[1]).legend_.remove()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6.15 We would also like to draw the line plot to show the trending of certain letters through years.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nsns.relplot(data=endingcount[endingcount.ending.isin(['d', 'n', 'y'])],\n            x='year', y='prop', hue='ending', kind='line')",
    "crumbs": [
      "Part I: Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Projects with Python</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#exercises",
    "href": "contents/6/intro.html#exercises",
    "title": "6  Projects with Python",
    "section": "6.3 Exercises",
    "text": "6.3 Exercises\n\nExercise 6.1 Please use the baby name dataset. We would like to consider the diversity of the names. Please compute the number of popular names in top 50% for each year each gender. Draw a line plot to show the trend and discuss the result.\n\n\nExercise 6.2 Please consider the baby name dataset. Please draw the trends of counts of names ending in a, e, n across years for each gender.\n\n\n\n\n\n[1] McKinney, W. (2017). Python for data analysis: Data wrangling with pandas, NumPy, and IPython. O’Reilly Media.",
    "crumbs": [
      "Part I: Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Projects with Python</span>"
    ]
  }
]