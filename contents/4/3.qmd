
## Data Wrangling

### Tidy data


The same underlying data can be represented in multiple ways. To better study the data, it is better to make these data *tidy*.

::: {#def-}
A dataset is *tidy* if

1. Each variable have its own column.
2. Each observation have its own row.
3. Each value have its oven cell.
:::



::: {.callout-note collapse="true"}
# Typical examples of tidydata

These `DataFrame` are provided by `tidyr`. We will talk about them again when we get to R. These tables can be downloaded by clicking the names.


1. [`table1`](assests/datasets/table1.csv)


```{python}
#| echo: true
import pandas as pd
table1 = pd.read_csv('assests/datasets/table1.csv', index_col='Unnamed: 0')
table1
```

2. [`table2`](assests/datasets/table2.csv)

```{python}
#| echo: true
import pandas as pd
table2 = pd.read_csv('assests/datasets/table2.csv', index_col='Unnamed: 0')
table2
```

3. [`table3`](assests/datasets/table3.csv)

```{python}
#| echo: true
import pandas as pd
table3 = pd.read_csv('assests/datasets/table3.csv', index_col='Unnamed: 0')
table3
```


4. Spread across two `DataFrame`s: [`table4a`](assests/datasets/table4a.csv) and [`table4b`](assests/datasets/table4b.csv):

```{python}
#| echo: true
import pandas as pd
table4a = pd.read_csv('assests/datasets/table4a.csv', index_col='Unnamed: 0')
table4b = pd.read_csv('assests/datasets/table4b.csv', index_col='Unnamed: 0')
table4a
```

```{python}
#| echo: true
table4b
```


Among all these `DataFrame`s, only `table1` is tidy.

:::



These three conditions are interrelated because it is impossible to only satisfy two of the three. In pratical, we need to follow the instructions:

1. Put each dataset in a `DataFrame`.
2. Put each variable in a column.
3. Every row is about one obeservation.

*Tidy* data is a consistent way to organize your data. The main advantages are:

1. It is one consistent way of storing data. In other words, this is a consistent data structure that can be used in many cases.
2. To placing variables in columns enables Python to do vectorized operations.



Most datasets are untidy, since tidy data is usually not intuitive for collecting. Therefore raw data which are collected by some naive ideas are usually not tidy. 

Untidy data are usually:

- One variable might be spread across multiple columns.
- One observation might be scattered across multiple rows.



::: {.callout-note collapse="true"}
# `.melt()` method

A common problem is that the column names are not names of variables, but values of a variable. For example, `table4a` above has columns `1999` and `2000`. These two names are actually the values of a variable `year`. In addition, each row represents two observations, not one.

```{python}
#| echo: false
table4a
```
To tidy this type of dataset, we need to gather those columns into a new pair of variables. We need three parameters:

- The set of columns that represent values. In this case, those are `1999` and `2000`.
- The name of the variable. In this case, it is `year`. 
-The name of the variable whose values are spread over the cells. In this case, it is the number of `cases`. 

Then we apply `.melt()`.

```{python}
table4a.melt(id_vars=['country'],
             value_vars=['1999', '2000'],
             var_name='year',
             value_name='cases')
```


We can do the similar thing to `table4b`. 

```{python}
table4b.melt(id_vars=['country'],
             value_vars=['1999', '2000'],
             var_name='year',
             value_name='population')
```


::: {.callout-tip}
In Python there are multiple different ways to change a wide `DataFrame` to be longer like `.melt()`. Among all of them, `.melt()` is the most common one.
:::

:::



::: {.callout-note collapse="true"}
# `.pivot()` method


Another issuse is that an observation is scattered across multiple rows. Take `table2` as an example. 

An observation is a country in a year, but each observation is spread across two rows.

```{python}
table2
```
We could apply `.pivot()` to make it tidy. Here we need two arguments.

- The column that contains variable names. Here, it’s `type`.
- The column that contains values forms multiple variables. Here, it’s `count`.


```{python}
table2.pivot(index=['country', 'year'], columns='type', values='count')
```


<!-- 


`pivot_wider()` is an updated approach to `spread()`, designed to be both simpler to use and to handle more use cases. We recommend you use `pivot_wider()` for new code; `spread()` isn't going away but is no longer under active development. -->

:::



::: {.callout-note collapse="true"}
# Split and combine columns

If we would like to split one columns into multiple columns since there are more than one values in a cell, we could use `Series` string method to split it. 


```{python}
table3['newrate'] = table3['rate'].str.split('/')
table3
```

If we prepare two columns from the beginning, we could directly get two columns. Note that the argument `expand=True` means that we want to get a `DataFrame` by expanding dimensionality. More details can be found [here](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html).

```{python}
table3[['cases', 'population']] = table3['rate'].str.split('/', expand=True)
table3.drop(columns=['rate', 'newrate'], inplace=True)
table3
```



Similarly we could also combine columns just as they are strings.

```{python}
table3['another_rate'] = table3['cases']+'/'+table3['population']
table3
```


:::




### Hierarchical indexing {#sec-hierindexing}
Pandas support a more complex indexing system, that the index may have multiple levels. See the following example.


::: {.callout-note collapse="true"}
# An example

```{python}
import pandas as pd
import numpy as np

data = pd.Series(np.random.randn(9),
                 index = [['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'],
                          [1, 2, 3, 1, 2, 3, 1, 2, 3]])
data
```
You may look at the Series using different levels of indexes.

```{python}
data['a']
```

```{python}
data.loc[:, 2]
```

You may use groupby to group by levels and do calculations related to levels. More `.groupby()` will be discussed in the next section. 

```{python}
data.groupby(level=1).sum()
```


:::




From the example above, you may notice that the 2-level hierarchical indexing for a Series works very similar to a DataFrame. In fact, you may translate it back and forth between a 2-level indexing Series and a DataFrame.

```{python}
df = data.unstack()
df
```

```{python}
df.stack()
```

For DataFrame the index for both axes can be multiindex. The usual indexing way can be used if you want to start from the first level of the index. The more specific method to extract data is `.xs`.



::: {.callout-note collapse="true"}
# An example

```{python}
import pandas as pd

df1 = pd.DataFrame(
    {
        "A": ["A0", "A1", "A2", "A3"],
        "B": ["B0", "B1", "B2", "B3"],
        "C": ["C0", "C1", "C2", "C3"],
        "D": ["D0", "D1", "D2", "D3"],
    },
    index=[0, 1, 2, 3],
)

df2 = pd.DataFrame(
    {
        "A": ["A4", "A5", "A6", "A7"],
        "B": ["B4", "B5", "B6", "B7"],
        "C": ["C4", "C5", "C6", "C7"],
        "D": ["D4", "D5", "D6", "D7"],
    },
    index=[4, 5, 6, 7],
)

df = pd.concat([df1, df2], keys=['x', 'y'])
```


```{python}
df
```

```{python}
df['A']
```

```{python}
df.loc['x']
```

```{python}
df.loc['x',3]
```
```{python}
df.xs(3, level=1, drop_level=False)
```
:::




### Combining and Merging Datasets
`merge` and `concat` are the two most common ways to combine datasets. 

::: {.callout-note collapse="true"}
# `pd.merge()` function
Merge combines datasets by linking rows using one or more keys. This is from relational databases (e.g., SQL-based). 

Here are some examples. 

::: {#exm-}

```{python}
import pandas as pd
df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],
                    'data1': range(7)})
df2 = pd.DataFrame({'key': ['a', 'b', 'd'], 'data2': range(3)})
```
The two DataFrames are displayed as follows.

```{python}
df1
```

```{python}
df2
```

```{python}
pd.merge(df1, df2, on='key')
```
If the column names are different in each object, you can specify them separately.

```{python}
df3 = pd.DataFrame({'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],
                    'data1': range(7)})
df4 = pd.DataFrame({'rkey': ['a', 'b', 'd'],
                    'data2': range(3)})
pd.merge(df3, df4, left_on='lkey', right_on='rkey')
```

:::

By default `merge` does an inner join, that the keys in the result are the interesection found in both tables. Below are different types of `merge`. To specify the method for merge, the option is `how`.

- `inner`
- `left`
- `right`
- `outer`

Let's see the following examples.




::: {.grid}

::: {.g-col-6}
```{python}
df1 = pd.DataFrame({'Key': [1, 2], 'A': [0, 2], 'B': [1, 3]})
df1
```
:::

::: {.g-col-6}
```{python}
df2 = pd.DataFrame({'Key': [1, 3], 'C': [0, 2], 'D': [1, 3]})
df2
```
:::

:::



::: {.grid}


::: {.g-col-6}
```{python}
pd.merge(df1, df2, on='Key', how='inner')
```
:::


::: {.g-col-6}
```{python}
pd.merge(df1, df2, on='Key', how='outer')
```
:::

:::



::: {.grid}


::: {.g-col-6}
```{python}
pd.merge(df1, df2, on='Key', how='left')
```
:::

::: {.g-col-6}
```{python}
pd.merge(df1, df2, on='Key', how='right')
```
:::

:::



::: {.callout-note}
If a key combination appears more than once in both tables, the resulting table will have the Cartesian product of the associated data. Here is a very basic example with one unique key combination.

```{python}
df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],
                    'data1': range(6)})
df2 = pd.DataFrame({'key': ['a', 'b', 'a', 'b', 'd'],
                    'data2': range(5)})
pd.merge(df1, df2, on='key', how='left')
```
:::


::: {.callout-note} 
If the merge keys in a DataFrame is in its index instead of column(s), we could pass `left_index=True` or `right_index=True` or both instead of setting `left_on`/`right_on`/`on`.
:::


::: {#exm-crossexample-deck}
If we want to really create a Cartesian product, we may use the option `how='cross'`. For example, we would like to generate a deck of cards, we may use the following codes.

```{python}
suit = pd.DataFrame({'suit': ['spades', 'hearts', 'clubs', 'diamonds']})
face = pd.DataFrame({'face': list(range(1, 14))})
deck = pd.merge(suit, face, how='cross')
```
:::



:::









::: {.callout-note collapse="true"}
# `pd.concat()` function
The `concat()` function (in the main pandas namespace) performs concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.  


```{python}
import pandas as pd

df1 = pd.DataFrame(
    {
        "A": ["A0", "A1", "A2", "A3"],
        "B": ["B0", "B1", "B2", "B3"],
        "C": ["C0", "C1", "C2", "C3"],
        "D": ["D0", "D1", "D2", "D3"],
    },
    index=[0, 1, 2, 3],
)

df2 = pd.DataFrame(
    {
        "A": ["A4", "A5", "A6", "A7"],
        "B": ["B4", "B5", "B6", "B7"],
        "C": ["C4", "C5", "C6", "C7"],
        "D": ["D4", "D5", "D6", "D7"],
    },
    index=[4, 5, 6, 7],
)

df3 = pd.DataFrame(
    {
        "A": ["A8", "A9", "A10", "A11"],
        "B": ["B8", "B9", "B10", "B11"],
        "C": ["C8", "C9", "C10", "C11"],
        "D": ["D8", "D9", "D10", "D11"],
    },
    index=[8, 9, 10, 11],
)

pd.concat([df1, df2, df3], keys=['x', 'y', 'z'])
```

The default way of `pd.concat()` is vertically. Note that it will check the column names. If the column names don't match, new columns will be created and `nan` values will be assigned. 

If you want to concatenate the DataFrame horizontally you need to add `axis=1` option.
Similarly, row index will be checked before concatenating. See the following example.

::: {#exm-}
```{python}
pd.concat([df1, df2, df3], axis=1)
```
:::



::: {#exm-}
Consider the deck example from @exm-crossexample-deck. This time we would like to use `pd.concat()` to get the result.

```{python}
suitlist = ['spades', 'hearts', 'clubs', 'diamonds']
facelist = list(range(1, 14))
decklist = [pd.DataFrame({'suit': suit, 'face': facelist}) for suit in suitlist]
deck = pd.concat(decklist, ignore_index=True)
```

:::
:::

